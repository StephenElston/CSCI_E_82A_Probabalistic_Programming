{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review of Probability\n",
    "\n",
    "## CSCI E-83\n",
    "## Stephen Elston\n",
    "\n",
    "Probability theory is at the core of probabilistic programming. Therefore, it is important to have a good understanding of the principles of probability theory. This lesson introduces you to the basic concepts you will need to know to tackle probabilistic programming. Specifically you will learn:\n",
    "\n",
    "1. The three axioms of probability.\n",
    "2. How to work with conditional probability and Bayes' rule. \n",
    "3. Factoring probability distributions using the chain rule of probability. \n",
    "3. Computing marginal distributions for inference. \n",
    "4. Applying the concepts of dependence and independence to factor distributions. \n",
    "\n",
    "As a first step run the code below to import the required packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Axioms of probability\n",
    "\n",
    "All probability distributions must have a certain properties, which we refer to as the **axioms of probability**. \n",
    "\n",
    "A discrete distribution is defined by its **probability mass** function, with nonzero values as discrete points. We can speak of a **set of events** within the **sample space** of all possible events. For discrete distributions the axioms then become:\n",
    "\n",
    "- Probability for any set, A, is bounded between 0 and 1:  \n",
    "\n",
    "$$0 \\le P(A) \\le 1 $$\n",
    "- For a discrete distribution, the sum of the probability over the Sample Space = 1:  \n",
    "\n",
    "$$P(S) = \\sum_{All\\ i}P(a_i) = 1 $$\n",
    "\n",
    "- The probability of finite independent unions is the sum of their probabilities:\n",
    "\n",
    "$$P(A \\cup B) = P(A) + P(B)\\\\ \n",
    "if\\ and\\ only\\ if\\\\ \n",
    "A \\cap B = 0 $$\n",
    "\n",
    "In contrast to discrete distributions, a **continuous distribution** is defined by its **probability density** function. For a continuous distribution the first and second axioms become:\n",
    "\n",
    "$$0 \\le P(a, b) = \\int_a^b P(x) dx  \\le 1$$\n",
    "\n",
    "and \n",
    "\n",
    "$$P(-\\infty, \\infty) = \\int_{-\\infty}^\\infty P(x) dx  = 1$$\n",
    "\n",
    "As a corollary of the first axiom, you can see that the probability over and infinitesimal interval must be zero:\n",
    "\n",
    "$$0 \\le P(a, a) = \\int_a^a P(x) dx  = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make these ideas concrete, let's try an example. The code in the cell below creates a data frame with the the probabilities of hair and eye color combinations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Black</th>\n",
       "      <th>Blond</th>\n",
       "      <th>Brunette</th>\n",
       "      <th>Red</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Brown</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Blue</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hazel</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Green</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Black  Blond  Brunette   Red\n",
       "Brown   0.11   0.01      0.21  0.04\n",
       "Blue    0.03   0.16      0.14  0.03\n",
       "Hazel   0.03   0.02      0.09  0.02\n",
       "Green   0.01   0.03      0.05  0.02"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eyeHair = pd.DataFrame({'Black':[0.11, 0.03, 0.03, 0.01], \n",
    "                     'Brunette':[0.21, 0.14, 0.09, 0.05],\n",
    "                     'Red':[0.04, 0.03, 0.02, 0.02],\n",
    "                     'Blond':[0.01, 0.16, 0.02, 0.03]}, \n",
    "                      index = ['Brown', 'Blue', 'Hazel', 'Green'])\n",
    "eyeHair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table contains a bivariate **joint distribution** of $p(hair,eye)$. By joint distribution we mean a probability distribution that depends on multiple variables; two in this case. For example, the probability of a subject in this sample having black hair and brown eyes: $p(black,brown) = 0.11$ .\n",
    "\n",
    "You can see that all of these probabilities are in the range $0 \\le p(hair,eye) \\le 1.0$, and therefore satisfy one of the axioms. \n",
    "\n",
    "We can test if these probabilities add up to 1.0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999999"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(eyeHair).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To within the rounding error, this probabilities add to 1.0 and satisfy another axiom. \n",
    "\n",
    "The question of independence or dependence is a bit more complicated, and will be addressed later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional distributions and Bayes' Theorem\n",
    "\n",
    "A probability distribution of one random variable can be conditionally dependent on another random variable. **Bayes' theorem**, also known as **Bayes' rule**, gives us a powerful tool to think about and analyze conditional probabilities. We can \n",
    "\n",
    "$$P(A \\cap B) = P(A|B)P(B)\\\\\n",
    "P(B \\cap A) = P(B|A)P(A)$$\n",
    "\n",
    "Now:\n",
    "\n",
    "$$P(A \\cap B) = P(B \\cap A)$$\n",
    "\n",
    "This leads to Bayes' theorem as follows:\n",
    "\n",
    "$$P(A|B)P(B) = P(B|A)P(A)\\\\\n",
    "P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$$\n",
    "\n",
    "Which is Bayes' theorem! \n",
    "\n",
    "How can we interpret Bayes' theorem? We can think of Bayes' theorem in the following terms:\n",
    "\n",
    "$$Posterior(hypotheis\\ |\\ evidence) = \\frac{Likelihood(evidence\\ |\\ hypothesis)\\ prior(hypothesis)}{\\sum_{h' \\in\\ All\\ possible\\ hypotheses}Likelihood(evidence\\ |\\ h')\\ prior(h')}$$\n",
    "\n",
    "In other words, we can find the posterior probability of a hypothesis using Bayes' theorem. Using the likelihood and prior probability of the hypothesis. Notice that the normalization is the sum over all possible hypotheses. This can be problematic in practice.  \n",
    "\n",
    "We can compute $p(eye | hair = Black)$. First we can find the probabilities from the joint distribution of $p(eye, hair = Black)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Brown    0.11\n",
       "Blue     0.03\n",
       "Hazel    0.03\n",
       "Green    0.01\n",
       "Name: Black, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eyeHair_black = eyeHair.loc[:,'Black']\n",
    "eyeHair_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These numbers cannot be a probability distribution as they do not add up to 1.0. However, normalizing is easy in this case. This gives us the **marginal probability distribution** of eye color given black hair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Brown    0.611111\n",
       "Blue     0.166667\n",
       "Hazel    0.166667\n",
       "Green    0.055556\n",
       "Name: Black, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eyeHair_black = eyeHair_black/sum(eyeHair_black)\n",
    "eyeHair_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above calculation is an example of **inference**. We have inferred the marginal distribution of eye color given black hair from the joint distribution. \n",
    "\n",
    "We can also find the mostly likely or most probable eye color given black hair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For pople with Black eyes: \n",
      "Most common hair color = Brown\n",
      "with probability = 0.61\n"
     ]
    }
   ],
   "source": [
    "print('For pople with Black eyes: \\nMost common hair color = ' + eyeHair_black.idxmax() + \n",
    "      '\\nwith probability = %4.2f' % (eyeHair_black.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain rule of probability\n",
    "\n",
    "Another way to work with distributions is by applying the **chain rule of probability**. This rule allows us to factor a joint distribution as follows:\n",
    "\n",
    "$$P(A_1, A_2, A_3, A_4 \\ldots, A_n) = P(A_1 | A_2, A_3, A_4, \\ldots, A_n)\\ P(A_2, A_3, A_4 \\ldots, A_n)$$\n",
    "\n",
    "In words, a joint distribution can be factored as a distribution of one of the variable, conditioned on the other variables, multiplied by the joint distribution of the other variables. \n",
    "\n",
    "We can continue this factorization until we reach an end point:\n",
    "\n",
    "$$P(A_1, A_2, A_3, A_4 \\ldots, A_n) = P(A_1 | A_2, A_3, A_4, \\ldots, A_n)\\ P(A_2 | A_3, A_4 \\ldots, A_n)\\ P(A_3| A_4 \\ldots, A_n) \\ldots p(A_n)$$\n",
    "\n",
    "Or in general terms, we can expand a joint distribution as the product of conditional distributions:\n",
    "\n",
    "$$P(\\bigcap_{k=1}^n A_k) = \\prod_{k=1}^n p(A_k \\big| \\bigcap_{j=1}^{n-1} A_j)$$\n",
    "\n",
    "> **Note:** The factorization is not unique. We can factor the variables in any order. In fact, for a joint distribution with $n$ variables, there are $n!$ unique factorizations. For example, we can factorize the foregoing distribution as:\n",
    "\n",
    "$$P(A_1, A_2, A_3, A_4 \\ldots, A_n) = P(A_n | A_{n-1}, A_{n-2}, A_{n-3}, \\ldots, A_1)\\ P(A_{n-1}| A_{n-2}, A_{n-3}, \\ldots, A_1)\\ P(A_{n-2}| A_{n-3}, \\ldots, A_1) \\ldots p(A_1)$$\n",
    "\n",
    "As an example of a factorization using the chain rule of probability, we can find the conditional distribution of eye color given hair color (or the reverse). Our table of data only gives us the joint distributions which we can factor as:\n",
    "\n",
    "$$P(eye,hair) = P(eye|hair)\\ P(hair) \\\\\n",
    "or,\\\\\n",
    "P(eye|hair) = \\frac{P(eye,hair)}{P(hair)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marginal distributions\n",
    "\n",
    "Inference for probabilistic models is typically performed by computing **marginal distributions**. A marginal distribution is the probability distribution of one or more variables summed or integrated over the other variables of a multivariate distribution. This process is often an essential step in performing **inference**. By inference, we mean returning the results of a query on a variable of a probabilistic model. \n",
    "\n",
    "For example, if we start with a joint distribution we can factor it using the chain rule of probabilities:\n",
    "\n",
    "$$p(A,B) = P(A|B)p(B)$$\n",
    "\n",
    "And, we can compute the marginal distribution over $A$ by summing over $B$:\n",
    "\n",
    "$$P(A) = \\sum_{B} P(A|B)p(B) $$\n",
    "\n",
    "The concept is simple. The the result of the summation is the distribution on the *margin* of the multivariate distribution. \n",
    "\n",
    "Using our dataset of hair and eye color, it is easy to compute the marginal probabilities of eye color as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Black</th>\n",
       "      <th>Blond</th>\n",
       "      <th>Brunette</th>\n",
       "      <th>Red</th>\n",
       "      <th>MarginalEye</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Brown</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Blue</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hazel</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Green</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Black  Blond  Brunette   Red  MarginalEye\n",
       "Brown   0.11   0.01      0.21  0.04         0.37\n",
       "Blue    0.03   0.16      0.14  0.03         0.36\n",
       "Hazel   0.03   0.02      0.09  0.02         0.16\n",
       "Green   0.01   0.03      0.05  0.02         0.11"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eyeHair['MarginalEye'] = eyeHair.sum(axis = 1)\n",
    "eyeHair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute the marginal probability of hair color:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Black</th>\n",
       "      <th>Blond</th>\n",
       "      <th>Brunette</th>\n",
       "      <th>Red</th>\n",
       "      <th>MarginalEye</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Brown</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Blue</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hazel</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Green</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MarginalHair</th>\n",
       "      <td>0.18</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.11</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Black  Blond  Brunette   Red  MarginalEye\n",
       "Brown          0.11   0.01      0.21  0.04         0.37\n",
       "Blue           0.03   0.16      0.14  0.03         0.36\n",
       "Hazel          0.03   0.02      0.09  0.02         0.16\n",
       "Green          0.01   0.03      0.05  0.02         0.11\n",
       "MarginalHair   0.18   0.22      0.49  0.11         1.00"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eyeHair = pd.concat([eyeHair, pd.DataFrame({'MarginalHair':eyeHair.sum(axis = 0)}).T])\n",
    "eyeHair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many cases we really only want to know the maximum of the marginal probability. For example, we can find the most probable eye color:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Brown'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eyeHair.iloc[:3,4].idxmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While brown eyes are  the most probable, blue eyes are nearly as likely. \n",
    "\n",
    "Likewise, we can find the most probable hair color.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Brunette'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eyeHair.iloc[4,:3].idxmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More on Bayes' theorem\n",
    "\n",
    "We have already compute the probability of eye color given hair color in a previous section. Let's check that we get the same result using Bayes' theorem. This is another form of inference. \n",
    "\n",
    "By the chain rule of probabilities the joint distribution,  $P(hair,eye)$ , can be factored as:\n",
    "\n",
    "$$P(hair,eye) = P(hair|eye)\\ P(eye)\\\\\n",
    "Or,\n",
    "P(hair|eye) = \\frac{P(hair,eye)}{P(eye)}$$\n",
    "\n",
    "Now using Bayes' rule, we can write:\n",
    "\n",
    "$$P(eye | hair) = \\frac{P(hair|eye)\\ P(eye)}{P(hair)} = \\frac{P(hair,eye)\\ P(eye)}{P(hair) \\ P(eye)} = \\frac{P(hair,eye)}{P(hair)}$$\n",
    "\n",
    "The quantities $P(eye)$ and $P(hair)$ are just the marginal distributions. So, we can easily compute the conditional distribution of eye color give black hair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Brown    0.611111\n",
       "Blue     0.166667\n",
       "Hazel    0.166667\n",
       "Green    0.055556\n",
       "Name: Black, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PEyeGivenHair_black = eyeHair.loc[:,'Black'][:4]/eyeHair.loc['MarginalHair','Black']\n",
    "PEyeGivenHair_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can check that the result is a proper distribution by verifying the sum adds to 1.0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of probabilites of eye given black hair = 1.00\n"
     ]
    }
   ],
   "source": [
    "print('Sum of probabilites of eye given black hair = %4.2f' % sum(PEyeGivenHair_black)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above can be summarized by rewriting Bayes'theorem as follows:\n",
    "\n",
    "$$P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\\\\\n",
    "= \\frac{P(B|A)P(A)}{\\sum_A P(A,B)}\\\\\n",
    "= \\frac{P(B|A)P(A)}{\\sum_A P(B|A)\\ P(A)}\\\\\n",
    "= \\frac{P(B|A)P(A)}{Marginal\\ distribution\\ of\\ B}$$\n",
    "\n",
    "In summry we can write the demominator in Bayes' theorm as a marginal distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independence and dependence\n",
    "\n",
    "The **independence** structure is required to fully specify a probability distribution. Further, being able to specify an independence structure can greatly simplify the factorization of a distribution. On the other hand, **dependence** limits the ability to factorize a distribution.    \n",
    "\n",
    "If $A$ and $B$ are independent then:\n",
    "\n",
    "$$P(A|B) = P(A) \\Leftrightarrow P(B|A) = P(B)$$\n",
    "\n",
    "Consider the joint distribution $P(A,B)$. If $A$ is independent of $B$ then we can apply the chain rule of probability:\n",
    "\n",
    "$$P(A,B) = P(A|B)P(B) = P(B|A)P(A) = P(A)P(B)$$\n",
    "\n",
    "With the above relationships in mind, let's revisit the chain rule of probability: \n",
    "\n",
    "$$P(A_1, A_2, A_3, A_4 \\ldots, A_n) = P(A_1 | A_2, A_3, A_4, \\ldots, A_n)\\ P(A_2 | A_3, A_4 \\ldots, A_n)\\ P(A_3| A_4 \\ldots, A_n) \\ldots p(A_n)$$\n",
    "\n",
    "If all of $A_1, A_2, A_3, A_4 \\ldots, A_n$ are all independent, then the chain rule of probability becomes:  \n",
    "\n",
    "$$P(A_1, A_2, A_3, A_4 \\ldots, A_n) = P(A_1)\\ P(A_2)\\ P(A_3) \\ldots p(A_n)$$\n",
    "\n",
    "This factorization leads to the **naive Bayes model**. The model is considered *naive*, since no dependency between the variables is accounted for.   \n",
    "\n",
    "As another example consider $P(A,B,C,D)$ where \n",
    "- $A$ is dependent on $C$,\n",
    "- $B$ is dependent on $D$,\n",
    "- $A$ is independent of $B$ and $D$, \n",
    "- $B$ is independent of $C$, and\n",
    "- $C$ is independent of $D$. \n",
    "\n",
    "We can write these **conditional independence** relationships as:\n",
    "\n",
    "$$P(A \\bot B,D| C) \\\\\n",
    "P(B \\bot A, C| D) \\\\\n",
    "P(C \\bot D)$$\n",
    "\n",
    "In words, some variables are independent of other given the dependence on other. The symbol $\\bot$ indicates independence. \n",
    "\n",
    "Using these conditional independences we can simplify the factorization of the distribution as follows:\n",
    "\n",
    "$$P(A,B,C,D) = P(A|B,C,D)\\ P(B|C,D)\\ P(C|D)\\ P(D) = P(A|C)\\ P(B|D)\\ P(C)\\ P(D)$$\n",
    "\n",
    "Notice how this independence/dependence structure simplifies the factorization of the distribution.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Law of total probability\n",
    "\n",
    "The **law of total probability** is another useful tool for factorizing probability distributions. You can think of the law of total probability an extension of the idea of marginal distributions. \n",
    "\n",
    "Let's start with a conditional distribution $P(A|B,C)$ where $B \\bot C$. Then we can factorize and simplify the distribution as:\n",
    "\n",
    "$$P(A\\ |\\ C) = \\sum_B P(A\\ |\\ B,C)\\ P(B\\ |\\ C)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another example\n",
    "\n",
    "As she approaches graduation a student is searching for a job as a machine learning engineer. In order to get the job she is interested in she must submit both GRE scores and a letter of recommendation from her machine learning professor. Unfortunately, the professor has a poor memory and will base his letter only on the grade the student received.  \n",
    "\n",
    "The student would like to compute the joint distribution $P(D,I,S,G,L)$. She can make the following **assertions** about the dependencies and independencies for this problem. \n",
    "\n",
    "1. The degree of difficulty of the machine learning course is **unconditionally independent** of all other variable, $\\{D \\bot I,S,G.L \\}$.\n",
    "2. The intelligence of the student is **unconditionally independent** of all other variable, $\\{I \\bot D,S,G.L \\}$.\n",
    "3. The student's letter is **conditionally independent**  of intelligence, her GRE score, and her letter, given her grade, $\\{L \\bot I,S,L\\ |\\ G\\}$.\n",
    "4. The student's grade is **conditionally independent** of her GRE score and her letter, give the difficulty of the course and her intelligence, $\\{G  \\bot S,L\\ |\\ I,D\\}$. \n",
    "5. The students GRE scores are **conditionally independent** of her grade, difficulty of the course and letter, given her intelligence, $\\{S \\bot G,D,L\\ |\\ I\\}$.\n",
    "\n",
    "Let's factorize the distribution, keeping in mind that the factorization is not unique. Given the assertions we find:\n",
    "\n",
    "$$P(L,G,S,I,D) = P(D)\\ P(I)\\ P(S|I)\\ P(G|D,I)\\ P(L|G)$$\n",
    "\n",
    "\n",
    "Let's assign some **prior probabilities** to these distributions, Starting with intelligence, I, which is independent of all other variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I0</th>\n",
       "      <th>I1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    I0   I1\n",
       "I  0.2  0.8"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I = pd.DataFrame({'I':[0.2,0.8]}, index = ['I0','I1'])\n",
    "I.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difficulty of the course is independent of all of variables as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>D0</th>\n",
       "      <th>D1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>D</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    D0   D1\n",
       "D  0.3  0.7"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = pd.DataFrame({'D':[0.3,0.7]}, index = ['D0','D1'])\n",
    "D.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GRE score, given intelligence, is a 2x2 table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S0</th>\n",
       "      <th>S1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I0</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I1</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      S0    S1\n",
       "I0  0.95  0.05\n",
       "I1  0.10  0.90"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GREGivenI = pd.DataFrame({'S0':[0.95,0.1],'S1':[0.05,0.9]}, index = ['I0','I1'])\n",
    "GREGivenI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the letter given the grade is a 2d, 3x2 column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>L0</th>\n",
       "      <th>L1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>G1</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G2</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G3</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      L0    L1\n",
       "G1  0.10  0.90\n",
       "G2  0.40  0.60\n",
       "G3  0.99  0.01"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = pd.DataFrame({'L0':[0.1,0.4,0.99],'L1':[0.9,0.6,0.01]}, index = ['G1','G2','G3'])\n",
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The situation with the course grade is a bit more complicated. The student's grade is dependent on both her intelligence and the difficulty of the course. We can represent this distribution using a Pandas `MultiIndex`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiIndex(levels=[['I0', 'I1'], ['D0', 'D1']],\n",
      "           labels=[[0, 0, 1, 1], [0, 1, 0, 1]],\n",
      "           names=['I', 'D'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>G1</th>\n",
       "      <th>G2</th>\n",
       "      <th>G3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I</th>\n",
       "      <th>D</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">I0</th>\n",
       "      <th>D0</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D1</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">I1</th>\n",
       "      <th>D0</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D1</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         G1    G2    G3\n",
       "I  D                   \n",
       "I0 D0  0.30  0.40  0.30\n",
       "   D1  0.05  0.25  0.70\n",
       "I1 D0  0.90  0.08  0.02\n",
       "   D1  0.50  0.30  0.20"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = [['I0','I0','I1','I1'], ['D0','D1','D0','D1']]\n",
    "idx_tuple = list(zip(*idx))\n",
    "index = pd.MultiIndex.from_tuples(idx_tuple, names=['I', 'D'])\n",
    "print(index)\n",
    "G = pd.DataFrame({'G1':[0.3,0.05,0.9,0.5],'G2':[0.4,0.25,0.08,0.3],'G3':[0.3,0.7,0.02,0.2]}, \n",
    "                 index = index)\n",
    "G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all of the above, notice that the rows add to 1.0. This must be the case for these representations to be valid joint distributions.   \n",
    "\n",
    "The joint distributions can be computed by multiplication of these variables. For example, we can multiply the GRE score given intelligence by probability of intelligence:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S0</th>\n",
       "      <th>S1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I0</th>\n",
       "      <td>0.19</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I1</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      S0    S1\n",
       "I0  0.19  0.01\n",
       "I1  0.08  0.72"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GREGivenI.mul(I.iloc[:,0], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that these probabilities add up to 1.0. For this student with a high probability of being intelligent the most likely outcome is a good letter. \n",
    "\n",
    "Let's introduce some **evidence** into this problem. Let's say we know for a fact that the student is intelligent. Now we can easily compute the marginal distribution of her GRE score, which is just a row of the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S0    0.1\n",
       "S1    0.9\n",
       "Name: I1, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GREGivenI.loc['I1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, our student has a high probability of getting a good GRE score. \n",
    "\n",
    "Let's say that the machine learning course is, not too surprisingly, known to be difficult, another piece of **evidence**. We can compute the marginal probability of the students grade given these two bits of evidence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "G1    0.5\n",
       "G2    0.3\n",
       "G3    0.2\n",
       "Name: (I1, D1), dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.loc['I1','D1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our student has a 50% probability of getting a top grade, even in the difficult course. \n",
    "\n",
    "What the student really wants to know is if she will get a good letter or not. We can compute the joint distribution of letter and grade:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>G1</th>\n",
       "      <th>G2</th>\n",
       "      <th>G3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>L0</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L1</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      G1    G2     G3\n",
       "L0  0.05  0.12  0.198\n",
       "L1  0.45  0.18  0.002"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DistLetter = L.transpose() * G.loc['I1','D1']\n",
    "DistLetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What the student really wants to know is the marginal distribution of letter, which is now easily calculated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "L0    0.368\n",
       "L1    0.632\n",
       "dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DistLetter.sum(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, our student has a good chance of getting a high GRE score (0.9) and a reasonable chance of getting a good letter (0.63). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One more example\n",
    "\n",
    "The following example shows how we can expand the numerator and denominator when applying Bayes' theorem. The critical, and often difficult, part is to determine the probabilities of all of the alternative hypothesis summed to compute the numerator. \n",
    "\n",
    "Inspector Andrea Markov receives a call that Lord Fauntleroy has been murdered in his manor house. She is told that only the cook and butler were present in the house at the time of the crime and that there is no evidence of an intruder. Further, she learns that the only possible murder weapons are a knife and poison. \n",
    "\n",
    "The Inspector realizes that the cook and the butler could have acted alone or together Therefore, we will assume independence: $p(C,B) = p(C)p(B)$, where $p(C)$ is the probability the cook is guilty and $p(B)$ is the probability the butler is guilty.   \n",
    "\n",
    "How can Inspector Markov compute a prior probability of the four possible cases, $p(B|K)$, $p(B|P)$, $p(C|K)$ and $p(C|P)$, where $K = knife$ and $P = poison$. Let's start with Bayes' Theorem for the first case:\n",
    "\n",
    "$$p(B|K) = \\frac{p(K|B)\\ p(B)}{p(K)}$$\n",
    "\n",
    "We can reformulate this relationship as the calculation of a marginal distribution:\n",
    "$$p(B|K) = \\sum_C p(B,C|K) \\\\\n",
    "or,\\\\\n",
    "= \\sum_C \\frac{p(B,C,K)}{P(k)} \\\\\n",
    "= \\frac{\\sum_C p(K|B,C)\\ P(B,C)}{\\sum_{C,B,W} p(W|B,C)\\ P(B,C)}$$\n",
    "\n",
    "Notice that the numerator or normalization is summed over all variables to compute the **marginal distribution** of a knife being the murder weapon, $p(K)$.     \n",
    "\n",
    "Now, keeping in mind that $p(C,B) = p(C)p(B)$ we can write this relationship:\n",
    "\n",
    "$$p(B|K) = \\frac{p(B)\\ \\sum_C p(K|B,C)\\ P(C)}{\\sum_B P(B)\\ \\sum_{C,W} p(W|B,C)\\ P(C)} \\\\\n",
    "= \\frac{p(B)\\ \\sum_C p(K|B,C)\\ P(C)}{\\sum_B P(B)\\ \\sum_C P(C)\\ \\sum_{W} p(W|B,C)}$$\n",
    "\n",
    "The factorization of the numerator is not unique. We can also write:\n",
    "\n",
    "$$p(B|K)\n",
    "= \\frac{p(B)\\ \\sum_C p(K|B,C)\\ P(C)}{\\sum_C P(C)\\ \\sum_B P(B)\\ \\sum_{W} p(W|B,C)}$$\n",
    "\n",
    "So, the numerator is the same for both the case of the butler or the cook being the murderer. \n",
    "\n",
    "Before we expand this relationship, we need to introduce some notation. Let $\\wedge$ be the logical AND operator and $\\urcorner$ be the logical NOT operator. Using this notation we can expand the numerator:\n",
    "\n",
    "$$p(B)\\ \\sum_C p(K|B,C)\\ P(C) \\\\\n",
    "= p(B) \\big[ p(C)\\ p(K|B \\wedge C) +  p( \\urcorner C)\\ p(K|B \\wedge \\urcorner C) \\big]$$\n",
    "\n",
    "This does not look too bad. \n",
    "\n",
    "The denominator is quite a bit more complicated as there are 8 possible cases. \n",
    "\n",
    "$$\\sum_B P(B)\\ \\sum_C P(C)\\ \\sum_{W} p(W|B,C) $$   \n",
    "   \n",
    "Now, the probability of poison has been determined to be zero; $p(P) = 0$. This reduces the number of possible cases to 8.  Additionally, since there are only two possible suspects, $p(\\urcorner B \\wedge \\urcorner C) = 0$. The two cases (one for each possible weapon) where neither butler or cook committed the crime must be zero. We can now expand the denominator with these simplifications as follows:\n",
    "    \n",
    "$$= p(B) \\Big[ p(C)\\ \\big( p(K|B \\wedge C) + p(P|B \\wedge C)\\big)  + p( \\urcorner C)\\ \\big(  p(K|B \\wedge \\urcorner C) + p(P|B \\wedge \\urcorner C)  \\big) \\Big] \\\\\n",
    "+ p(\\urcorner B) \\Big[ p(C)\\ \\big( p(K| \\urcorner B \\wedge C) + p(P| \\urcorner B \\wedge C) \\big) \\Big]$$\n",
    "\n",
    "In summary, the denominator sums the probabilities of all possible alternative hypothesis. Details for the other cases are left to you as an exercise. \n",
    "\n",
    "As advertised there are six none-zero probability alternatives in the denominator. Consider what would happen if some the variables had more than two possible states. For example, how many alternatives are there if there were four possible murder weapons. Or, how many states are created if we added 2 addition suspects as well as four possible murder weapons. These though experiments should lead you to understand that computational complexity grows rather quickly (combinatorically) with the number of states.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copyright 2018, Stephen F. Elston. All rights reserved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
