{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Bayesian Networks (BNs)\n",
    "\n",
    "\n",
    "\n",
    "## CSCI E-83\n",
    "## Stephen Elston\n",
    "\n",
    "**Bayesian Networks** or **BNs** are one of the basic representations of probabilistic graphical models. BNs are **directed acyclic graphs** or **DAGs** which form a compact representation of a joint distribution. \n",
    "\n",
    "The representation captures the **conditional independence structure** of the joint distribution. Taking advantage of the independency structure of the distribution can greatly reduce the computational complexity of inference on the graphic model. \n",
    "\n",
    "The directed nature of the BN can capture **causality**. **Influence** travels along the directed edges of the graph. However, keep in mind, information on independence can flow both directions along the graph.\n",
    "\n",
    "The **conditional marginal distribution** of each variable is only dependent on its **parents**. In other words, **belief** is propagated along the DAG. In this sense, the DAG is a representation of a **generative sampling process**. \n",
    "\n",
    "Graphical models allow us to add **evidence** as data is observed for certain variables. Further, we can perform **queries** on **hidden variables**, variables with no evidence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some graph terminology\n",
    "\n",
    "Before we dive into the probabilistic graphical models let's examine some basic graph terminology. Graphs are are comprised of two types of elements:\n",
    "1. **Nodes** or **vertices** in probabilistic models contain the distribution information.\n",
    "2. **Edges** define the relationships between the nodes. Edges can be **directed** or **undirected**. Information flows in the direction specified along a directed edge. Undirected edges have no preferred direction of information flow. \n",
    "3. The set of edges if a graph (less nodes) is called the **skeleton**. \n",
    "4. Nodes connected by an edge are **neighbors**. \n",
    "5. The **degree** of a node is the number of neighbors. \n",
    "6. **Parents** are nodes with a directed edge to another node, know as the **child**. Parents are said to have **influence** on their children. \n",
    "7. **Ancestors** are nodes which are grandparents, great grandparents, etc, of a with a path of directed edges to a child. \n",
    "8. **Descendants** are nodes which receive information from parents or ancestors along a path of directed edges. For example, a child node is a descendant of its parents and grand parents. \n",
    "9. **Leaf** nodes have no directed edges to ancestors. \n",
    "\n",
    "We can classify graphs by their topology. The figure below shows some basic graph types.   \n",
    "\n",
    "<img src=\"img/GraphTypes.JPG\" alt=\"Drawing\" style=\"width:600px; height:200px\"/>\n",
    "<center> **Common ** </center>\n",
    "\n",
    "Four graph types are illustrated in the figure above:   \n",
    "1. **Directed acyclic graph** is a graph where all edges are **directed** and there are no **cycles**. Information or **influence** flows in one direction along a directed edge. In the case illustrated, influence flows  $A \\rightarrow B$ and $C \\rightarrow B$, but not the other way. This makes $A$ and $C$ **ancestors** or **parents** of $B$. An **acyclic graph** has no cycle.\n",
    "2. **Directed cyclic graph** has directed edges but contains a cycle. A **cycle** is a directed path that starts and returns to the same node. In this case the illustrated case the cycle is $A \\rightarrow B \\rightarrow C \\rightarrow A$.\n",
    "3. **Undirected graph** has no directed edges. \n",
    "4. **Partially directed graph** has both directed and undirected edges. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple example\n",
    "\n",
    "We have already worked on the student GRE score and letter problem. As a reminder, a student would like to make inferences or **queries** on the joint distribution $P(D,I,S,G,L)$. She can make the following assertions about the independencies for this problem: \n",
    "\n",
    "1. The degree of difficulty, D, of the machine learning course is **unconditionally independent** of all other variable, $\\{D \\bot I,S,G.L \\}$.\n",
    "2. The intelligence of the student is, I, **unconditionally independent** of all other variable, $\\{I \\bot D,S,G.L \\}$.\n",
    "3. The quality of the student's recommendation letter, L, is **conditionally independent**  of intelligence, her GRE score, and her letter, given her grade, $\\{L \\bot I,S,L\\ |\\ G\\}$.\n",
    "4. The student's grade in the machine learning course, G, is **conditionally independent** of her GRE score and her letter, give the difficulty of the course and her intelligence, $\\{G  \\bot S,L\\ |\\ I,D\\}$. \n",
    "5. The students GRE scores, S, are **conditionally independent** of her grade, difficulty of the course and letter, given her intelligence, $\\{S \\bot G,D,L\\ |\\ I\\}$.\n",
    "\n",
    "Given these independecies, the distribution can be factored as follows:\n",
    "\n",
    "$$P(D,I,S,G,L) = p(D)\\ p(I)\\ p(S|I)\\ p(G|I,D)\\ P(L|G)$$\n",
    "\n",
    "It is relatively easy to construct a BN that represents the independencies of this distribution as shown in the figure below.  \n",
    "\n",
    "<img src=\"img/LetterDAG.JPG\" alt=\"Drawing\" style=\"width:600px; height:300px\"/>\n",
    "<center> **DAG for the student score and letter distribution** </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independency and uniqueness of BNs\n",
    "\n",
    "We have just defined a BN with the Independency structure of the distribution defined by the assertions. The independency structure is quite important since by imposing independencies can greatly reduce the computational complexity of the graph.   \n",
    "\n",
    "However, there is no guarantee that a given BN uniquely defines the independency structure of a distribution. A simple example of four BNs representing the same independency structure is shown in the figure below. In each case, the following **conditional independence** assertions are true:\n",
    "\n",
    "$$A\\ \\bot\\ C\\ |\\ B\\\\\n",
    "Or,\\\\\n",
    "C\\ \\bot\\ A\\ |\\ B$$\n",
    "\n",
    "\n",
    "<img src=\"img/Dependency.JPG\" alt=\"Drawing\" style=\"width:600px; height:300px\"/>\n",
    "<center> **Multiple BNs with same dependence structure** </center>\n",
    "\n",
    "Each of these cases supports that   \n",
    "\n",
    "1. **Causal or Cascade:** In this case A causes B which causes C. This information is represented in the directed edges. If B is an evidence variable then A and C are separated and cannot dependent. Once a value is assigned to B, A depends on B and C depends on B, but A and C are **decoupled**. \n",
    "2. **Evidential:** Once again, the evidence variable B separates A and C, just as they do in the causal case. In this case, evidence is added to the DAG which allows inference of A.   \n",
    "3. **Common Evidence, V-Structure, or Collider:** In this case, having evidence on B **blocks** the path or **separates** A from C. In other words, knowing B explains away A and C. \n",
    "4. **Common Cause or Common Parent:** In this case, B is causal to both A and C, making A and C independent. \n",
    "\n",
    "A key point here is that any of these DAGs has the same aforementioned independence properties. In other words, multiple DAGs can exhibit the same independencies. We has that these DAGs exhibit **I-equivalence**, since they have the same independencies. \n",
    "\n",
    "We can write a generalization of the independence properties of a BN:\n",
    "\n",
    "> **Definition:** On a graph $G$, the variable $X_i$ is independent of its **nondecendents** given its **parents**, $Pa_{X_i}$. We can say that $G$ includes as set of **local conditional independence assumptions**:\n",
    "\n",
    "$$I_{\\ell}(G): \\{X_i\\ \\bot\\ Nondecendants_{X_i}\\ |\\ Pa_{X_i}:\\ \\forall i\\ \\}$$\n",
    "\n",
    "Let's look at some more examples of applying this definition The Figure below illustrates some key cases.\n",
    "\n",
    "<img src=\"img/Independencies2.JPG\" alt=\"Drawing\" style=\"width:600px; height:350px\"/>\n",
    "<center> **Multiple BNs with same dependence structure** </center>\n",
    "\n",
    "Following the numbering of each DAG, we can make make the following assertions (statements) about independencies in each of these DAGs:\n",
    "\n",
    "1. **$P(A,B,C) = P(B\\ |\\ A,C)\\ P(A)\\ P(C)$:** In this case, A and C are causes of B, and are therefore independent.\n",
    "2. **$P(A,B,C) = P(A\\ |\\ B)\\ P(C\\ |\\ B)\\ P(B)$:** Here, B is the cause of independent effects A and C.\n",
    "3. **$P(A,C\\ |\\ B) = P(A\\ |\\ B)\\ P(C\\ |\\ B)$:** A and C are dependent given B. While the causes of A and C are a-priori independent, having evidence for B tells us about both causes A and C. \n",
    "4. **$P(A,C\\ |\\ B) = P(A\\ |\\ B)\\ P(C\\ |\\ B)$:** A and C are independent when conditioned on B. Given the cause B we know everything about each effect A and C. \n",
    "5. **$P(A,C\\ |\\ B) = P(A\\ |\\ B)\\ P(C\\ |\\ B)$:** Since D is a descendant of B, this case is the same as 3 above.\n",
    "6. **$P(A,C) = P(A)\\ P(C)$:** A and C are made independent by marginalizing over B. In other works, in the absence of specific information on B, A and C are unconditionally independent.\n",
    "7. **$P(A,C) \\ne P(A)\\ P(C)$:** In this case, A and C are dependent when marginalizing over B. In other words the effects, A and C are dependent on B. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pgmpy for directed graphical model\n",
    "\n",
    "We can use the Python pgmpy package to create and manipulate Bayesian DAGs. You can find complete documentation on this package [here](http://pgmpy.org/).\n",
    "\n",
    "If you have not installed pgmpy, un-comment the code in the cell below and execute it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pgmpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To import the parts of pgmpy we will use for the examples in this notebook, execute the code in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.factors.discrete import TabularCPD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing with the example, we need to define the graph. The first step is to define the **skeleton** of the graph. This is done by defining the edges. Each edge is define as a tuple; (start node, end node). Execute the code in the cell below to create the skeleton of the graph      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the network structure.\n",
    "## The first value of the tuple defines the origin\n",
    "## of the connector and the second the terminal point\n",
    "## of the directed edge. \n",
    "student_model = BayesianModel([('D', 'G'), ('I', 'G'), ('G', 'L'), ('I', 'S')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the graph skeleton defined, we need to specify the probability distributions of the nodes. In this case, we are using discrete distributions.   \n",
    "\n",
    "We will start with the two independent variables, $D$ and $I$, which are binomially distributed. Notice that the sum of the probabilities must add to 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═════╤═════╕\n",
      "│ D_0 │ 0.7 │\n",
      "├─────┼─────┤\n",
      "│ D_1 │ 0.3 │\n",
      "╘═════╧═════╛\n",
      "╒═════╤═════╕\n",
      "│ I_0 │ 0.8 │\n",
      "├─────┼─────┤\n",
      "│ I_1 │ 0.2 │\n",
      "╘═════╧═════╛\n"
     ]
    }
   ],
   "source": [
    "## Define the independent variables\n",
    "CDP_D = TabularCPD(variable='D', variable_card=2, values=[[0.7, 0.3]])\n",
    "print(CDP_D)\n",
    "CDP_I = TabularCPD(variable='I', variable_card=2, values=[[0.8, 0.2]])\n",
    "print(CDP_I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will deal with the children of the two independent variables. Now we need to define the variables associated with **evidence variables**. An evidence variable is an **observable** variable for which data can be acquired. We will explore observable variables and evidence later. \n",
    "\n",
    "The letter variable, $L$, is dependent on the grade $G$. Therefore we say that $G$ is the **evidence** variable. In a similar manner we can model the GRE score, $S$, using the evidence variable, Intelligence, $I$. Execute the code in the cell below and examine the tables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═════╤═════╤═════╤══════╕\n",
      "│ G   │ G_0 │ G_1 │ G_2  │\n",
      "├─────┼─────┼─────┼──────┤\n",
      "│ L_0 │ 0.1 │ 0.4 │ 0.99 │\n",
      "├─────┼─────┼─────┼──────┤\n",
      "│ L_1 │ 0.9 │ 0.6 │ 0.01 │\n",
      "╘═════╧═════╧═════╧══════╛\n",
      "╒═════╤══════╤═════╕\n",
      "│ I   │ I_0  │ I_1 │\n",
      "├─────┼──────┼─────┤\n",
      "│ S_0 │ 0.95 │ 0.2 │\n",
      "├─────┼──────┼─────┤\n",
      "│ S_1 │ 0.05 │ 0.8 │\n",
      "╘═════╧══════╧═════╛\n"
     ]
    }
   ],
   "source": [
    "# Define the distributions with a single conditional variable \n",
    "# or evidence. \n",
    "CDP_L = TabularCPD(variable='L', variable_card=2, \n",
    "                   values=[[0.1, 0.4, 0.99],\n",
    "                           [0.9, 0.6, 0.01]],\n",
    "                   evidence=['G'], # Leter depends on the grade\n",
    "                   evidence_card=[3])\n",
    "print(CDP_L)\n",
    "\n",
    "CDP_S = TabularCPD(variable='S', variable_card=2,\n",
    "                   values=[[0.95, 0.2],\n",
    "                           [0.05, 0.8]],\n",
    "                   evidence=['I'], # GRE score depneds on intelligence\n",
    "                   evidence_card=[2])\n",
    "print(CDP_S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one last variable, Grade, $G$. There are two evidence variables in this case, Intelligence, $I$ and Difficulty, $D$. This relationship leads to a **three dimensional table**. We can represent this relationship using multiple indicies on one axis. Execute the code in the cell below and examine the resulting table.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═════╤═════╤══════╤══════╤═════╕\n",
      "│ I   │ I_0 │ I_0  │ I_1  │ I_1 │\n",
      "├─────┼─────┼──────┼──────┼─────┤\n",
      "│ D   │ D_0 │ D_1  │ D_0  │ D_1 │\n",
      "├─────┼─────┼──────┼──────┼─────┤\n",
      "│ G_0 │ 0.3 │ 0.05 │ 0.9  │ 0.5 │\n",
      "├─────┼─────┼──────┼──────┼─────┤\n",
      "│ G_1 │ 0.4 │ 0.25 │ 0.08 │ 0.3 │\n",
      "├─────┼─────┼──────┼──────┼─────┤\n",
      "│ G_2 │ 0.3 │ 0.7  │ 0.02 │ 0.2 │\n",
      "╘═════╧═════╧══════╧══════╧═════╛\n"
     ]
    }
   ],
   "source": [
    "# Define the distributions with a multiple conditional variables \n",
    "# or evidence. \n",
    "CDP_G = TabularCPD(variable='G', variable_card=3, \n",
    "                   values=[[0.3, 0.05, 0.9,  0.5],\n",
    "                           [0.4, 0.25, 0.08, 0.3],\n",
    "                           [0.3, 0.7,  0.02, 0.2]],\n",
    "                  evidence=['I', 'D'],\n",
    "                  evidence_card=[2, 2])\n",
    "print(CDP_G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one last step. We need to add the CDPs to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<TabularCPD representing P(D:2) at 0x291c67d07f0>,\n",
       " <TabularCPD representing P(I:2) at 0x291c67d0908>,\n",
       " <TabularCPD representing P(S:2 | I:2) at 0x291c67d0e10>,\n",
       " <TabularCPD representing P(G:3 | I:2, D:2) at 0x291c67d0a90>,\n",
       " <TabularCPD representing P(L:2 | G:3) at 0x291c67d0dd8>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_model.add_cpds(CDP_D, CDP_I, CDP_S, CDP_G, CDP_L)\n",
    "student_model.get_cpds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now defined the directed graphical model using three steps:\n",
    "1. Skeleton of the graph with the directed edges,\n",
    "2. **conditional probability distributions** (**CPD**) for each node, and\n",
    "3. the evidence variables for each node. \n",
    "\n",
    "With the graph defined we can find the independencies of the DAG representing the distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(D _|_ I, L, S, G)\n",
       "(I _|_ D, L, S, G)\n",
       "(S _|_ L, D, G | I)\n",
       "(G _|_ L, S | D, I)\n",
       "(L _|_ D, I, S | G)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Find the local independencies in the BN\n",
    "# Getting all the local independencies in the network.\n",
    "student_model.local_independencies(['D', 'I', 'S', 'G', 'L'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These independencies are exactly as expected. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAGs and independence maps\n",
    "\n",
    "We can generalize beyond the above example we can say that a DAG represents a probability distribution **consistent** with the DAG. We can factor the the graph:\n",
    "$$P(X) = \\prod_{i=1:d}P(x_i|X_{\\pi_i})$$\n",
    "\n",
    "Where, $P(X)$ is probability distribution, $X_{\\pi_i}$ is the set of **parents** of $x_i$, a node on the graph, and $d$. In other words, in a DAG a node $x_i$ is independent of all other nodes give the set of the node's parents, $X_{\\pi_i}$.\n",
    "\n",
    "We can also generalize the idea of independencies with the **local Markov assumption** applied to a DAG G by writing:\n",
    "\n",
    "$$I(G) = \\{ X \\bot Z\\ |\\ Y:\\ dsep_G(X:Z|Y) \\} $$ \n",
    "\n",
    "This leads us to an important definition of the properties of DAGs:\n",
    "\n",
    "> **Definition:** A DAG, G, is an **independence map** or **I-map** of a distribution P if $I_l(G) \\subseteq I(P)$, where $I(P)$ is the set of independencies of the distribution P and $I_l(G)$ is the set of independencies of the DAG. We can express this relationship as:\n",
    "\n",
    "$$(X\\ \\bot\\ Y\\ |\\ Z_G) \\Rightarrow (X\\ \\bot\\ Y\\ |\\ Z_P)$$\n",
    "\n",
    "We can see that there are multiple DAGs for which $I_l(G) \\subseteq I(P)$ can be true.  But, what are the practical implications of this definition? \n",
    "\n",
    "From the above definition, you can see that it can be the case that a DAG can be an I-map, but not a complete representation of the independencies of $P$. This leads us to another definition:\n",
    "\n",
    ">**Definition:** A DAG, G, is a **minimal I-map** for a distribution P if removal of even a single edge renders $G$ not an I-map. \n",
    "\n",
    "The concept of an I-map is quite useful. In fact, no graph $G$ can be considered a useful representation of a distribution $P$ if it is not an I-map of $P$. A minimal I-map will have the minimum possible complexity, but may be hard to find in practice. \n",
    "\n",
    "Note that a minimal I-map may not be unique for a distribution $P$. If two DAGs have the same I-map, we say they are **I-equivalent**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trails in DAGs\n",
    "\n",
    "\n",
    "The concept of **trails** provides another way to think about separation in DAGs. You can think of a trail as a path between one subset of a DAG and another subset of the DAG. If the trail is **active** there is no independency between the  If a trail between subsets is **blocked** the subsets are independent. Trails are blocked by evidence or observed. Following on the previous figure we can define four cases:\n",
    "\n",
    "1. **Causal trail;** is active if and only if B is not observed. \n",
    "2. **Evidential trail;** is active if and only if B is not observed.\n",
    "3. **Common cause;** is active if and only if B is not observed.\n",
    "4. **Common effect;** is active if and only if B or one of B's decendants is observed.  \n",
    "\n",
    "The independencies resulting from trails can be generalized s **d-separation**:\n",
    "\n",
    "> **Definition:** Let $X$, $Y$ and $Z$ be three subsets of graph G. We say $X$ and $Y$ are **d-separated** given Z if there is no active trail between and node $x \\in X$ and any othee node $y \\in Y$ given Z. We can express d-separation as $d-sep_G(X;Y\\ |\\ Z)$.   \n",
    "\n",
    "The intuition is that when there is evidence on the trail between subsets, the subset with evidence effectively blocks the trail creating the separation.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active trail between D and G -> True\n",
      "Active trail between G and S -> True\n",
      "Active trail between D and I -> False\n",
      "Active trail between L and I -> True\n",
      "Active trail between L and S -> True\n"
     ]
    }
   ],
   "source": [
    "def test_active(start,end):\n",
    "    print('Active trail between ' + start + ' and ' + end + ' -> ' + str(student_model.is_active_trail(start,end)))\n",
    "starts = ['D', 'G', 'D', 'L', 'L']\n",
    "ends = ['G', 'S', 'I', 'I', 'S']\n",
    "for s,e in zip(starts,ends): test_active(s,e)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evidence, hidden variables and queries\n",
    "\n",
    "Up to now we have only dealt with distributions of the variables in our graphical models. \n",
    "\n",
    "It is possible that we will **observe** data values for some variables, which is typically referred to as **evidence**. Evidence provides specific values for **observable variables**. Once we have evidence, the size of the tables we must work with can be reduced. Only entries consistent with the evidence need be retained. Thus, computational complexity can be reduced, sometimes greatly. \n",
    "\n",
    "An example of evidence might be a specific score $S$ on the GRE test for a student. If the score is high, $S^1$, then we need not consider table entries with a low GRE score $S^0$.\n",
    "\n",
    "Not all variables are observable however. We must estimate the marginal distribution of these **unobservable variables** given the evidence. We say that we perform a **query** on the unobservable variables. The query returns the marginal distribution of the variable. \n",
    "\n",
    "For example, human intelligence cannot be directly observed. In our student model, we might observe a student's GRE score as $S^1$ and course grade as $G^0$. Given this evidence, we can now query to compute the marginal posterior distribution of the student's intelligence. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_model.get_cardinality('G')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<TabularCPD representing P(D:2) at 0x291c67d07f0>,\n",
       " <TabularCPD representing P(I:2) at 0x291c67d0908>,\n",
       " <TabularCPD representing P(S:2 | I:2) at 0x291c67d0e10>,\n",
       " <TabularCPD representing P(G:3 | I:2, D:2) at 0x291c67d0a90>,\n",
       " <TabularCPD representing P(L:2 | G:3) at 0x291c67d0dd8>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_model.get_cpds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncertain and unreliable evidence\n",
    "\n",
    "In the real world, evidence can be **uncertain** or **unreliable**. Both of these situations effect the marginal distribution returned by a query on any unobservable variable. \n",
    "\n",
    "### Uncertain evidence\n",
    "\n",
    "First, let's consider the effect of **uncertain evidence**. There are many situations in which uncertain evidence can arise. For example, an observer might only have a certain confidence in an observation. An ornithologist sees a bird, but may only be 70% certain of the species. The quality of the habitat (an unobservable) is a parent of species present. How can we include the uncertainty of the ornithologist's observation into the model? \n",
    "\n",
    "A model of habitat quality, $q$, given the species, $s$ can be written:\n",
    "\n",
    "$$p(q\\ |\\ s) = \\sum_s p(q,s\\ |\\ \\tilde{s}) = \\sum_s p(q\\ |\\ s, \\tilde{s})\\ p(s\\ |\\ \\tilde{s})$$ \n",
    "\n",
    "Now, $q$ is dependent on $s$ independent of the uncertainty of the observation $\\tilde{s}$, leading to:\n",
    "\n",
    "$$\\sum_s p(q\\ |\\ s)\\ p(s\\ |\\ \\tilde{s})$$\n",
    "\n",
    "### Unreliable evidence\n",
    "\n",
    "Let's say our ornithologist decides her observations are going too slowly and decides to hire an assistant. Unfortunately, the person she hires has exaggerated his sill at bird species identification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge engineering with DAGs\n",
    "\n",
    "We have seen how DAGs are a powerful and compact representation of joint distributions. But, how do we specify the DAG? There are two distinct aspects of the specification process:\n",
    "\n",
    "1. The **qualitative specification** which defines the skeleton of the DAG. The skeleton specifies the influences in the belief network. There are a number of ways the qualitative specification of a DAG including: \n",
    "  - **Prior knowledge** of influence or causal relationships based on knowledge of the probability distributions in the domain. \n",
    "  - **Eliciting assessments of experts** as to which variables are influential or causal to other variables. \n",
    "  - **Learning** the independence structure of the probability distribution from data. This is a computationally difficult problem involving .      \n",
    "      \n",
    "      \n",
    "2. The **quantitative specification** which defines the details of the conditional distributions. There are a number of ways to create the quantitative specification of a DAG: \n",
    "  - **Prior knowledge** of the conditional distributions.\n",
    "  - **Empirically determining the conditional distributions** from data. \n",
    "  - **Eliciting opinions of experts** as to the likelihood of events. \n",
    "  - **Fitting** parameters of a distribution model.    \n",
    "     \n",
    "  \n",
    "It should be noted that multiple methods are often used to create these specifications.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copyright 2018, Stephen F Elston. All rights reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
