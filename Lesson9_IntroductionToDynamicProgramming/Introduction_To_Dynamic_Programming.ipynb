{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Dynamic Programming\n",
    "\n",
    "## CSCI E-82A\n",
    "## Stephen Elston\n",
    "\n",
    "In the previous lesson we explored the concepts of **Markov decision processes (MDP)** and **Markov reward processes (MRP)**. Now we will extend these concepts and apply them to finding **optimal solutions** for such systems. By an optimal solution, we mean a solution which produces **greater reward** than any other solution. \n",
    "\n",
    "To understand how we can find optimal solutions we must introduce some new concepts:\n",
    "1. An **action** causes a state transition. This transition may be to the same state. \n",
    "2. A **policy** specifies the **actions** given the state. In other words, the policy defines the actions to be specified by the agent. \n",
    "3. An **optimal policy** produces the greatest reward possible given the initial state of the system.\n",
    "4. A **plan** is the sequence of actions leading to an optimal result. \n",
    "\n",
    "In this an subsequent lessons, we will explore some powerful methods for finding optimal policies. Broadly, these methods are known as **dynamic programming** and **reinforcement learning**. In this lesson we will focus on the representation and learning methods for dynamic programming. Dynamic programming algorithms can be the basis of effective and flexible intelligent agents as shown below.\n",
    "\n",
    "<img src=\"img/DPAgent.JPG\" alt=\"Drawing\" style=\"width:500px; height:300px\"/>\n",
    "<center> **Dynamic Programming Agent and Environment** </center>  \n",
    "\n",
    "\n",
    "**Suggested readings** for dynamic programming are:\n",
    "- Sutton and Barto, second edition, Sections 3.7, 3.8, and Chapter 4 or,   \n",
    "- Russell and Norvig, third edition, Sections 17.1, 17.2 or, \n",
    "- Kochenderfer, Sections 4.1, 4.2, 4.3, 4.5, 4.6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Dynamic Programming?\n",
    "\n",
    "What is **dynamic programming**? In the most general terms, dynamic programming is a **optimal planning method**. Planning methods enable an intelligent agent to gain improved autonomy though a sequence of optimal actions to achieve a **goal**. \n",
    "\n",
    "Dynamic programming was developed in the 1950's by mathematician [**Richard Bellman**](https://en.wikipedia.org/wiki/Richard_E._Bellman). By *programming* Bellman meant a computer algorithm which computes a a plan of actions to optimize the **utility** or **total reward** for the states visited in a Markov process. By *dynamic*, Bellman meant the algorithm solves the problem recursively by operating on smaller and simpler sub-problems. \n",
    "\n",
    "Like dynamic programming, **reinforcement learning** is class of optimization algorithms using a sequence of actions for a system represented by a Markov processes.Therefore, understanding dynamic programming is good path to understanding reinforcement learning. \n",
    "\n",
    ". **Note:** If you are familiar with machine learning, you may recognize Bellman as the person who coined the phrase *the curse of dimensionality*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Functions and Policy Evaluation\n",
    "\n",
    "Given a Markov random process, a reward function, and a policy of actions, how can we evaluate the policy? We can perform **policy evaluation** in two ways, using a value function or an action value function.  \n",
    "\n",
    "### Policy Evaluation with Value Functions\n",
    "\n",
    "First, we can perform **policy evaluation** with a **value function**. The value function is the expected value of the **gain** achieved by following a policy, $\\pi(a|s)$, given the current state. We can express the state value function as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "v_{\\pi}(s) &= \\mathbb{E}_{\\pi} [ G_t\\ |\\ S_t = s] \\\\\n",
    "&= \\mathbb{E}_{\\pi} [R_{t+1} + \\gamma G_{t+1}\\ |\\ S_t = s] \\\\\n",
    "&= \\mathbb{E}_{\\pi} [R_{t+1} + \\gamma v_{\\pi} (S_{t+1})\\ |\\ S_t = s] \\\\\n",
    "&= \\sum_a \\pi(a|s) \\sum_{s',r} p(s',r | s,a) \\big[ r + \\gamma v_{\\pi}(s') \\big],\\ \\forall a \n",
    "\\end{align}\n",
    "$$\n",
    "Where,  \n",
    "$\\mathbb{E}_{\\pi} [] =$ Expectation given policy $\\pi$,   \n",
    "$v_{\\pi}(s) =$ value of state, s, given policy $\\pi$,   \n",
    "$G_t =$ return at step $t$,    \n",
    "$S_t =$ state at step $t$,    \n",
    "$R_t =$ reward at step $t$,  \n",
    "$\\pi(a|s) =$ the policy specifying action, $a$, given state, $s$,  \n",
    "$p(s',r | s,a) =$ probability of successor state, $s'$, and reward, $r$, given state, $s$, and action $a$,  \n",
    "$\\gamma =$ discount rate. \n",
    "\n",
    "\n",
    "The above relations are known as the **Bellman value equations**. This relationship tells us how to compute the value of being in a particular state, $s$. There is one such equation for each state, $s \\in \\xi$, of the Markov process.\n",
    "\n",
    "In principle, we could find the state values by solving the $\\xi \\in R^n$ simultaneous linear Bellman state value equations. However, this approach has computational complexity $O(n^3)$. \n",
    "\n",
    "Examine the last line of the above relations and notice it can be viewed as a **recursion**. This leads us to an iterative solutions. We can start with an initial set of state values (e.g, 0) and iterate the equation through $T$ time steps, until meeting a **convergence criteria**. The convergence criteria is a measure in the change of the state values from one iteration to the next. \n",
    "\n",
    "The recursion of the state value equations iterate though values of the successor states. The estimate of state value is improved at each step using= the estimates of successor states. We call such an algorithm a **Bootstrap method**. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Evaluation with Action Value Functions\n",
    "\n",
    "As an alternative to the state value function, we can use the **Bellman state action function**:\n",
    "\n",
    "$$\\begin{align}\n",
    "q_\\pi(s,a) &= \\mathbb{E_\\pi} \\big[G_{t}\\ \\big|\\ S_t = s, A_t = a \\big]  \\\\\n",
    "&= \\mathbb{E_\\pi} \\Big[ \\sum_{k=0}^\\infty \\gamma^k\\ R_{t+k+1} \\big|\\ S_t = s, A_t = a \\Big]  \\\\\n",
    "&= \\sum_{s',r} p(s',r\\ |\\ s,a) \\big[ r + \\gamma\\ q_\\pi(S_{t+1},a') \\big]\n",
    "\\end{align}$$\n",
    "\n",
    "Where,\n",
    "$q_\\pi(s,a) =$ is the action value, which is the value of action, $a$ from state $s$ following policy $\\pi$,   \n",
    "$A_t = $ the action taken at step $t$,  \n",
    "$a' = $ the action taken from the successor state, $s'$,  \n",
    "and other notation is the same as discussed for the state value function. \n",
    "\n",
    "\n",
    "Examining the last line of the above relationship shows that this is a recursion relation as well. Thus, we can bootstrap with the action value function to iteratively find the action values for the Markov process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Value for Grid World Example\n",
    "\n",
    "Let's try an example of computing the state values of a Markov process. In this example, we will work with both the **representations** and **learning**. We will apply the state value function approach here. \n",
    "\n",
    "**Navigation** to a goal is a significant problem in robotics. Real-world navigation is rather complex. Therefore, in this example we will use a simple analog called a **grid world**. The grid world for this problem is shown below. \n",
    "\n",
    "<img src=\"img/GridWorld.JPG\" alt=\"Drawing\" style=\"width:200px; height:200px\"/>\n",
    "<center> **A 4x4 Grid World with Terminal State** </center>\n",
    "\n",
    "The grid world consists of a 4x4 set of positions the robot can occupy. Each position is considered a state. The goal is to navigate to state 0, the goal, in the minimum steps. We will explore methods to find policies which reach this goal and achieve maximum reward. \n",
    "\n",
    "Grid position 0 is the goal and a **terminal state**. There are no possible state transitions out of this position. The presence of a terminal state makes this an **episodic Markov random process**. In each episode the robot can start in any other random position, $\\{ 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 \\}$, and the episode terminates when the robot enters the terminal position (state 0).  \n",
    "\n",
    "In each state, there are four possible actions the robot can take:\n",
    "- up, u\n",
    "- down, d,\n",
    "- left, l\n",
    "- right, r\n",
    "\n",
    "We encode, or represent, these possibilities in a dictionary as shown in the code block below. We use a dictionary of dictionaries to perform the lookup. The keys of the outer dictionary are the identifiers (numbers) of the states. The keys of the inner dictionary are the possible actions and the values are the **successor state**, $s'$, for that transition.  \n",
    "\n",
    "Notice that there are no allowed transitions out of the terminal state. Also, any transition that might take the robot off the grid, leaves the state unchanged. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import numpy for latter\n",
    "import numpy as np\n",
    "\n",
    "## Define the transition dictonary of dictionaries:\n",
    "policy = {0:{'u':0, 'd':0, 'l':0, 'r':0},\n",
    "          1:{'u':1, 'd':5, 'l':0, 'r':2},\n",
    "          2:{'u':2, 'd':6, 'l':1, 'r':3},\n",
    "          3:{'u':3, 'd':7, 'l':2, 'r':3},\n",
    "          4:{'u':0, 'd':8, 'l':4, 'r':5},\n",
    "          5:{'u':1, 'd':9, 'l':4, 'r':6},\n",
    "          6:{'u':2, 'd':10, 'l':5, 'r':7},\n",
    "          7:{'u':3, 'd':11, 'l':6, 'r':7},\n",
    "          8:{'u':4, 'd':12, 'l':8, 'r':9},\n",
    "          9:{'u':5, 'd':13, 'l':8, 'r':10},\n",
    "          10:{'u':6, 'd':14, 'l':9, 'r':11},\n",
    "          11:{'u':7, 'd':15, 'l':10, 'r':11},\n",
    "          12:{'u':8, 'd':12, 'l':12, 'r':13},\n",
    "          13:{'u':9, 'd':13, 'l':12, 'r':14},\n",
    "          14:{'u':10, 'd':14, 'l':13, 'r':15},\n",
    "          15:{'u':11, 'd':15, 'l':14, 'r':15}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define the initial transition probabilities for the Markov process. We set the probabilities for each transition as a **uniform distribution** leading to random action by the robot. As there are 4 possible transitions from each state, this means all transition probabilities are 0.25. In other words, this is a random policy which does not favor any particular plan. \n",
    "\n",
    "The initial uniform transition probabilities are encoded using a dictionary of dictionaries. The organization of this data structure is identical to the foregoing data structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_to_state_probs = {0:{'u':0.0, 'd':0.0, 'l':0.0, 'r':0.0},\n",
    "                        1:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25}, \n",
    "                        2:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        3:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        4:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        5:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        6:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        7:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        8:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        9:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        10:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        11:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        12:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        13:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        14:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        15:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25}}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The robot receives the following rewards:\n",
    "- 10 for entering position 0. \n",
    "- -1 for attempting to leave the grid. In other words, we penalize the robot for hitting the edges of the grid.  \n",
    "- -0.1 for all other state transitions, which is the cost for the robot to move from one state to another. If we did not have this penalty, the robot could follow any random plan to the goal which did not hit the edges. \n",
    "\n",
    "We encode this reward in the same type of dictionary structure used for the foregoing structures.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = {0:{'u':0.0, 'd':0.0, 'l':0.0, 'r':0.0},\n",
    "          1:{'u':-1, 'd':-0.1, 'l':10.0, 'r':-0.1},\n",
    "          2:{'u':-1.0, 'd':-0.1, 'l':-0.1, 'r':-0.1},\n",
    "          3:{'u':-1.0, 'd':-0.1, 'l':-0.1, 'r':-1.0},\n",
    "          4:{'u':10.0, 'd':-0.1, 'l':-1.0, 'r':-0.1},\n",
    "          5:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-0.1},\n",
    "          6:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-0.1},\n",
    "          7:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-1.0},\n",
    "          8:{'u':-0.1, 'd':-0.1, 'l':-1.0, 'r':-0.1},\n",
    "          9:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-0.1},\n",
    "          10:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-0.1},\n",
    "          11:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-1.0},\n",
    "          12:{'u':-0.1, 'd':-1.0, 'l':-1.0, 'r':-0.1},\n",
    "          13:{'u':-0.1, 'd':-1.0, 'l':-0.1, 'r':-0.1},\n",
    "          14:{'u':-0.1, 'd':-1.0, 'l':-0.1, 'r':-0.1},\n",
    "          15:{'u':-0.1, 'd':-1.0, 'l':-0.1, 'r':-1.0}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have constructed a rather poor policy, which is just a random walk around the grid. Still, we can still measure the value of this policy. \n",
    "\n",
    "The function in the code below iterates over the Bellman value function to find the values of each state. The iteration continues until the convergence criteria is meet.  \n",
    "\n",
    "> **Note:** The code in this example takes advantage of the fact that there is only one possible successor state for each action. This means there is no need to sum over successor states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.92461824, -3.93184927, -6.48887002],\n",
       "       [ 0.92461824, -2.10995108, -4.95119803, -6.86735717],\n",
       "       [-3.93184927, -4.95119803, -6.5102566 , -7.87700873],\n",
       "       [-6.48887002, -6.86735717, -7.87700873, -8.96905736]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_state_value(pi, probs, reward, gamma = 1.0, theta = 0.01, display = False):\n",
    "    '''Function for policy evaluation  \n",
    "    '''\n",
    "    delta = theta\n",
    "    values = np.zeros(len(probs)) # Initialize the value array\n",
    "    while(delta >= theta):\n",
    "        v = np.copy(values) ## save the values for computing the difference later\n",
    "        for s in probs.keys():\n",
    "            temp_values = 0.0 ## Initial the sum of values for this state\n",
    "            for action in rewards[s].keys():\n",
    "                s_prime = pi[s][action]\n",
    "                temp_values = temp_values + probs[s][action] * (reward[s][action] + gamma * values[s_prime])\n",
    "            values[s] = temp_values\n",
    "            \n",
    "        ## Compute the difference metric to see if convergence has been reached.    \n",
    "        diffs = np.sum(np.abs(np.subtract(v, values)))\n",
    "        delta = min([delta, diffs])\n",
    "        if(display): \n",
    "            print('difference metric = ' + str(diffs))\n",
    "            print(values.reshape(4,4))\n",
    "    return values\n",
    "\n",
    "compute_state_value(policy, state_to_state_probs, rewards, theta = 0.1, display = False).reshape(4,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The array displayed above shows the value of being in a state (grid position) given the policy. You can see that the closer the position is to the goal, the higher the value. This intuitively makes sense.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Dynamic Programming - Policy Iteration\n",
    "\n",
    "We now have a method to evaluate a policy for a Markov process. But, the current random policy is rather poor. Fortunately, the Bellman equations give us several possible dynamic programming algorithms for **policy improvement**. \n",
    "\n",
    "Before we get too far here, we should define what exactly mean by policy improvement. Intuitively, an improved policy should yield greater total utility than a previous policy. More specifically an improved policy $\\pi'$ has the following relationship with an initial policy, $\\pi$:\n",
    "\n",
    "$$v_{\\pi'}(s) >= v_\\pi(s)$$\n",
    "\n",
    "We can also say that an optimal policy $\\pi_*$ must conform to the following:\n",
    "\n",
    "$$v_{*}(s) >= v_\\pi(s)\\ \\forall \\pi$$\n",
    "\n",
    "In words, the optimal policy has a value greater than or equal to all other possible policies. Notice there is **no guarantee of uniqueness**. There can be multiple optimal policies.   \n",
    "\n",
    "The key idea of dynamic programming is expressed by the **Bellman optimality equations**. There are two ways we can express this optimal relationship. We can find a relationship which is a solution to the **Bellman optimal state action equations**, denoted $q_{*}(s,a)$. Or, we can find a solution to the **Bellman optimal state value equations**, $v_{*}(s)$, shown here:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "v_{*}(s) &= max_\\pi v_\\pi(s) \\\\\n",
    "&= max_a\\ \\mathbb{E} [ G_{t+1} + \\gamma v_*(S_{t+1})\\ |\\ S_t = s, A_t = a] \\\\\n",
    "&= max_a\\ \\sum_{s',r} p(s',r\\ |\\ s,a) \\big[ r + \\gamma v_{*}(s') \\big]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Given the $max_a$ operation this equations are no longer linear. But as with the state value functions, the optimal state value equations are naturally recursive and amenable to iterative solutions.\n",
    "\n",
    "We can gain some insight into the iterative process for the Bellman optimal state value equations by studying their **backup diagram**. A backup diagram is read from top to bottom. States are shown as open circles and actions by filled circles. The backup diagram for the Bellman optimal state value iteration is shown below.\n",
    "\n",
    "<img src=\"img/ValueBackup.JPG\" alt=\"Drawing\" style=\"width:250px; height:200px\"/>\n",
    "<center> **Backup diagram of Bellman Optimal State Value Function** </center>\n",
    "\n",
    "Let's follow this backup diagram from top to bottom.\n",
    "- The Markov process starts in some initial state $s$. \n",
    "- We take the action $a$ that maximizes the state value.\n",
    "- This leads to successor states, $s'$ with reward $r$.\n",
    "\n",
    "Notice that this algorithm, like a dynamic programming methods, requires us to evaluate the value of all possible actions for a given state. You can see this is the way the max sweep is shown in the above diagram. We say that dynamic programming requires **full backups**. This fact puts a lower bound on the computational complexity of dynamic programming methods. \n",
    "\n",
    "The requirement to perform full backups lead Richard Bellman to declare the *curse of dimensionality*. That is as the number of states (dimensions) grows so goes the computational complexity of dynamic programming. None the less, dynamic programming is a surprisingly scalable method. We have seen how DP is much more efficient that direct solution methods. Further, DP algorithms have significantly lower complexity than the equivalent graph search algorithms or linear programming methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration for Grid World\n",
    "\n",
    "Let's try the policy iteration algorithm on the grid world example. The function in the cell below performs policy iteration by the following steps:\n",
    "1. The state values and policy are initialized to some arbitrary starting point.\n",
    "2. The policy with the maximum value is found for all possible actions by bootstrapping.\n",
    "3. The improvement in the policy is evaluated using the state value function to determine if convergence has been achieved. If not, continue with step 2.\n",
    "\n",
    "The code in the cell below implements this algorithm and applies it to the grid world. Execute this code and examine the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "diff = 128.1086520299247\n",
      "Current policy\n",
      "{0: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}, 1: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0}, 2: {'u': 0.0, 'd': 0.3333333333333333, 'l': 0.3333333333333333, 'r': 0.3333333333333333}, 3: {'u': 0.0, 'd': 0.5, 'l': 0.5, 'r': 0.0}, 4: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0}, 5: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}, 6: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}, 7: {'u': 0.3333333333333333, 'd': 0.3333333333333333, 'l': 0.3333333333333333, 'r': 0.0}, 8: {'u': 0.3333333333333333, 'd': 0.3333333333333333, 'l': 0.0, 'r': 0.3333333333333333}, 9: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}, 10: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}, 11: {'u': 0.3333333333333333, 'd': 0.3333333333333333, 'l': 0.3333333333333333, 'r': 0.0}, 12: {'u': 0.5, 'd': 0.0, 'l': 0.0, 'r': 0.5}, 13: {'u': 0.3333333333333333, 'd': 0.0, 'l': 0.3333333333333333, 'r': 0.3333333333333333}, 14: {'u': 0.3333333333333333, 'd': 0.0, 'l': 0.3333333333333333, 'r': 0.3333333333333333}, 15: {'u': 0.5, 'd': 0.0, 'l': 0.5, 'r': 0.0}}\n",
      "With state values\n",
      "[[ 0.         10.          8.82261553  8.36513351]\n",
      " [10.          9.10497156  8.41694102  8.11572316]\n",
      " [ 8.82261553  8.41694102  8.04101798  7.87394536]\n",
      " [ 8.36513351  8.11572316  7.87394536  7.77394536]]\n",
      "\n",
      "diff = 18.591347970075287\n",
      "Current policy\n",
      "{0: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}, 1: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0}, 2: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0}, 3: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0}, 4: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0}, 5: {'u': 0.5, 'd': 0.0, 'l': 0.5, 'r': 0.0}, 6: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0}, 7: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0}, 8: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0}, 9: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0}, 10: {'u': 0.5, 'd': 0.0, 'l': 0.5, 'r': 0.0}, 11: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0}, 12: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0}, 13: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0}, 14: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0}, 15: {'u': 0.5, 'd': 0.0, 'l': 0.5, 'r': 0.0}}\n",
      "With state values\n",
      "[[ 0.  10.   9.9  9.8]\n",
      " [10.   9.9  9.8  9.7]\n",
      " [ 9.9  9.8  9.7  9.6]\n",
      " [ 9.8  9.7  9.6  9.5]]\n",
      "\n",
      "diff = 0.0\n",
      "Current policy\n",
      "{0: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}, 1: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0}, 2: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0}, 3: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0}, 4: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0}, 5: {'u': 0.5, 'd': 0.0, 'l': 0.5, 'r': 0.0}, 6: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0}, 7: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0}, 8: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0}, 9: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0}, 10: {'u': 0.5, 'd': 0.0, 'l': 0.5, 'r': 0.0}, 11: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0}, 12: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0}, 13: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0}, 14: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0}, 15: {'u': 0.5, 'd': 0.0, 'l': 0.5, 'r': 0.0}}\n",
      "With state values\n",
      "[[ 0.  10.   9.9  9.8]\n",
      " [10.   9.9  9.8  9.7]\n",
      " [ 9.9  9.8  9.7  9.6]\n",
      " [ 9.8  9.7  9.6  9.5]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: {'d': 0.25, 'l': 0.25, 'r': 0.25, 'u': 0.25},\n",
       " 1: {'d': 0.0, 'l': 1.0, 'r': 0.0, 'u': 0.0},\n",
       " 2: {'d': 0.0, 'l': 1.0, 'r': 0.0, 'u': 0.0},\n",
       " 3: {'d': 0.0, 'l': 1.0, 'r': 0.0, 'u': 0.0},\n",
       " 4: {'d': 0.0, 'l': 0.0, 'r': 0.0, 'u': 1.0},\n",
       " 5: {'d': 0.0, 'l': 0.5, 'r': 0.0, 'u': 0.5},\n",
       " 6: {'d': 0.0, 'l': 1.0, 'r': 0.0, 'u': 0.0},\n",
       " 7: {'d': 0.0, 'l': 1.0, 'r': 0.0, 'u': 0.0},\n",
       " 8: {'d': 0.0, 'l': 0.0, 'r': 0.0, 'u': 1.0},\n",
       " 9: {'d': 0.0, 'l': 0.0, 'r': 0.0, 'u': 1.0},\n",
       " 10: {'d': 0.0, 'l': 0.5, 'r': 0.0, 'u': 0.5},\n",
       " 11: {'d': 0.0, 'l': 0.0, 'r': 0.0, 'u': 1.0},\n",
       " 12: {'d': 0.0, 'l': 0.0, 'r': 0.0, 'u': 1.0},\n",
       " 13: {'d': 0.0, 'l': 0.0, 'r': 0.0, 'u': 1.0},\n",
       " 14: {'d': 0.0, 'l': 1.0, 'r': 0.0, 'u': 0.0},\n",
       " 15: {'d': 0.0, 'l': 0.5, 'r': 0.0, 'u': 0.5}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "def policy_iteration(pi, probs, reward, gamma = 1.0, theta = 0.1, output = False):\n",
    "    delta = theta\n",
    "    v = np.zeros(len(probs))\n",
    "    state_values = np.zeros(len(probs))\n",
    "    current_policy = copy.deepcopy(probs)\n",
    "    while(delta >= theta): # Continue until convergence.\n",
    "        for s in probs.keys(): # Iterate over all states\n",
    "            temp_values = []\n",
    "            for action in rewards[s].keys(): # Iterate over all possible actions for the state\n",
    "                # Compute list of values given action from the state\n",
    "                s_prime = pi[s][action]\n",
    "                temp_values.append(current_policy[s][action] * (reward[s][action] + gamma * state_values[s_prime]))\n",
    "            \n",
    "            ## Find the max value and update current policy\n",
    "            max_index = np.where(np.array(temp_values) == max(temp_values))[0]\n",
    "            prob_for_policy = 1.0/float(len(max_index))\n",
    "            for i,action in enumerate(current_policy[s].keys()): \n",
    "                if(i in max_index): current_policy[s][action] = prob_for_policy\n",
    "                else: current_policy[s][action] = 0.0\n",
    "                \n",
    "        \n",
    "        ## Compute state values with new policy to determine if there is an improvement\n",
    "        ## Uses the compute_state_value function\n",
    "        state_values = compute_state_value(pi, current_policy, rewards, theta = .1)\n",
    "        diff = np.sum(np.abs(np.subtract(v, state_values)))\n",
    "        if(output): \n",
    "            print('\\ndiff = ' + str(diff))\n",
    "            print('Current policy')\n",
    "            print(current_policy)\n",
    "            print('With state values')\n",
    "            print(state_values.reshape(4,4))\n",
    "        \n",
    "        delta = min([delta, np.sum(np.abs(np.subtract(v, state_values)))])\n",
    "        v = np.copy(state_values) ## copy of state values to evaluate next iteration\n",
    "    return current_policy\n",
    "\n",
    "policy_iteration(policy, state_to_state_probs, rewards, gamma = 1.0, output = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we interpret this optimal plan? The plan specifies the optimal state transitions to travel from any state to the goal. For example, the diagram below shows the optimal paths from State 15 to the goal. Notice that these paths are not unique in this plan. \n",
    "\n",
    "<img src=\"img/OptimalPath.JPG\" alt=\"Drawing\" style=\"width:300px; height:300px\"/>\n",
    "<center> **Optimal Paths From State 15 to Goal** </center>\n",
    "\n",
    "Any of the 4 possible alternatives illustrated above are optimal:\n",
    "\n",
    "- 15 -> 11 -> 7 -> 6 -> 5 -> 1 -> 0\n",
    "- 15 -> 11 -> 7 -> 6 -> 5 -> 4 -> 0\n",
    "- 15 -> 14 -> 13 -> 9 -> 5 -> 1 -> 0\n",
    "- 15 -> 14 -> 13 -> 9 -> 5 -> 4 -> 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration for Policy Improvement\n",
    "\n",
    "Now, we will investigate using the Bellman **optimal state action equations** for policy improvement. In this context, the policy improvement relations are expressed as:\n",
    "\n",
    "$$q_{\\pi}(s, \\pi_{*}(s)) >= v_\\pi(s) \\forall \\pi$$\n",
    "\n",
    "In words, the optimal state action policy $\\pi_{*}(s)$ gives a value greater than or equal to all other state action policies. Once again, there is no guarantee of uniqueness. \n",
    "\n",
    "The Bellman optimal state action equations are expressed:\n",
    "\n",
    "$$\\begin{align}\n",
    "q_{*}(s,a) &= max_\\pi q(s,a)\\\\\n",
    "&= \\mathbb{E} \\big[R_{t+1} + \\gamma max_{a'}\\ q_{*}(S_{t+1},a')\\ \n",
    "\\big|\\ S_t = s, A_t = a \\big]  \\\\\n",
    "&= max_a \\sum_{s',r} p(s',r\\ |\\ s,a) \\big[ r + \\gamma\\ max_{a'}\\ q_{*}(S_{t+1},a') \\big]\n",
    "\\end{align}$$\n",
    "\n",
    "Where, $q_{*}(s,a)$ is the policy giving action, $a$, from a state, $s$ with an **optimal policy**, $\\pi_*$. As with the Bellman state action value equations, there is one such equation for each action value tuple, $(a,s)$.\n",
    "\n",
    "\n",
    "We can gain some understanding of how the recursion works by looking at the **backup diagram** shown below.\n",
    "\n",
    "<img src=\"img/ActionValueBackup.JPG\" alt=\"Drawing\" style=\"width:250px; height:200px\"/>\n",
    "<center> **Backups diagram of Bellman Optimal Action Value Function** </center>\n",
    "\n",
    "\n",
    "Starting at the top of the diagram:\n",
    "- The Markov process starts with a state-action tuple, $(s,a)$.\n",
    "- This state-action leads to successor states, $s'$ with reward $r$.\n",
    "- The successor action, $a'$, with the maximum expect value is selected. \n",
    "\n",
    "As with policy iteration, notice that value iteration requires a full backup at each step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration for Grid World\n",
    "\n",
    "Let's try the value iteration algorithm on the grid world example. The function in the cell below performs value iteration by the following steps:\n",
    "1. The state values and policy are initialized to some arbitrary starting point.\n",
    "2. For possible state actions, the maximum value for successor states is found.\n",
    "3. Policy improvement is measures to find if the algorithm has converged. If not steps 2 and 3 are repeated. \n",
    "3. Given the resulting optimal state actions, the optimal policy $\\pi_*$ is found by taking the maximum over possible actions for each successor state.\n",
    "\n",
    "The code in the cell below implements this algorithm and applies it to the grid world. Execute this code and examine the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference = 146.70000000000002\n",
      "[[ 0.  10.   9.9  9.8]\n",
      " [10.   9.9  9.8  9.7]\n",
      " [ 9.9  9.8  9.7  9.6]\n",
      " [ 9.8  9.7  9.6  9.5]]\n",
      "Difference = 0.0\n",
      "[[ 0.  10.   9.9  9.8]\n",
      " [10.   9.9  9.8  9.7]\n",
      " [ 9.9  9.8  9.7  9.6]\n",
      " [ 9.8  9.7  9.6  9.5]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: {'d': 0.25, 'l': 0.25, 'r': 0.25, 'u': 0.25},\n",
       " 1: {'d': 0.0, 'l': 1.0, 'r': 0.0, 'u': 0.0},\n",
       " 2: {'d': 0.0, 'l': 1.0, 'r': 0.0, 'u': 0.0},\n",
       " 3: {'d': 0.0, 'l': 1.0, 'r': 0.0, 'u': 0.0},\n",
       " 4: {'d': 0.0, 'l': 0.0, 'r': 0.0, 'u': 1.0},\n",
       " 5: {'d': 0.0, 'l': 0.5, 'r': 0.0, 'u': 0.5},\n",
       " 6: {'d': 0.0, 'l': 0.5, 'r': 0.0, 'u': 0.5},\n",
       " 7: {'d': 0.0, 'l': 0.5, 'r': 0.0, 'u': 0.5},\n",
       " 8: {'d': 0.0, 'l': 0.0, 'r': 0.0, 'u': 1.0},\n",
       " 9: {'d': 0.0, 'l': 0.5, 'r': 0.0, 'u': 0.5},\n",
       " 10: {'d': 0.0, 'l': 0.5, 'r': 0.0, 'u': 0.5},\n",
       " 11: {'d': 0.0, 'l': 0.5, 'r': 0.0, 'u': 0.5},\n",
       " 12: {'d': 0.0, 'l': 0.0, 'r': 0.0, 'u': 1.0},\n",
       " 13: {'d': 0.0, 'l': 0.5, 'r': 0.0, 'u': 0.5},\n",
       " 14: {'d': 0.0, 'l': 0.5, 'r': 0.0, 'u': 0.5},\n",
       " 15: {'d': 0.0, 'l': 0.5, 'r': 0.0, 'u': 0.5}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def value_iteration(pi, probs, reward, goal, gamma = 1.0, theta = 0.1, output = False):\n",
    "    delta = theta\n",
    "    v = np.zeros(len(probs))\n",
    "    state_values = np.zeros(len(probs))\n",
    "    current_policy = copy.deepcopy(probs)\n",
    "    while(delta >= theta):\n",
    "        for s in probs.keys(): # iteratve over all states\n",
    "            temp_values = []\n",
    "            ## Find the values for all possible actions in the state.\n",
    "            for action in rewards[s].keys():\n",
    "                s_prime = pi[s][action]\n",
    "                temp_values.append((reward[s][action] + gamma * state_values[s_prime]))\n",
    "            \n",
    "            ## Find the max value and update the value for the state\n",
    "            state_values[s] = max(temp_values)\n",
    "        ## Determine if convergence is achieved\n",
    "        diff = np.sum(np.abs(np.subtract(v, state_values)))\n",
    "        delta = min([delta, np.sum(np.abs(np.subtract(v, state_values)))])\n",
    "        v = np.copy(state_values)\n",
    "        if(output):\n",
    "            print('Difference = ' + str(diff))\n",
    "            print(state_values.reshape(4,4))\n",
    "    \n",
    "    ## Now we need to find the policy that makes max value state transitions\n",
    "    for s in current_policy.keys(): # iterate over all states\n",
    "        ## Find the indicies of maximum state transition values\n",
    "        ## There are two cases. \n",
    "        ## First, the special case of a state adjacent to the goal\n",
    "        ## In this case need to ensure the only possible transition is to the goal.\n",
    "        ## Start by creating a list of the adjacent states.\n",
    "        possible_s_prime = [pi[s][key] for key in current_policy[s].keys()]\n",
    "        ## Check if goal is adjacent, but state is not the goal.\n",
    "        if(goal in possible_s_prime and s != goal):\n",
    "            temp_values = []\n",
    "            for s_prime in current_policy[s].keys(): # Iterate over adjacent states\n",
    "                if(pi[s][s_prime] == goal):  ## account for the special case adjacent to goal\n",
    "                    temp_values.append(reward[s][s_prime])\n",
    "                else: temp_values.append(0.0) ## Other transisions have 0 value.\n",
    "        ## The other case is rather easy requires a lookup of the value of the \n",
    "        ## adjacent state and handled with a list comprehension.             \n",
    "        else: temp_values = [state_values[pi[s][s_prime]] for s_prime in pi[s].keys()]         \n",
    "                \n",
    "        ## Find the index for the state transistions with the largest values \n",
    "        ## May be more than one. \n",
    "        max_index = np.where(np.array(temp_values) == max(temp_values))[0]  \n",
    "        prob_for_policy = 1.0/float(len(max_index)) ## Probabilities of transition\n",
    "        for i, key in enumerate(current_policy[s]): ## Update policy\n",
    "            if(i in max_index): current_policy[s][key] = prob_for_policy\n",
    "            else: current_policy[s][key] = 0.0       \n",
    "    return current_policy\n",
    "\n",
    "value_iteration(policy, state_to_state_probs, rewards, goal = 0,  output = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting optimal plan is not the same as the one developed by policy iteration. There are more options, However, the utility of an set of state transitions from the initial state the goal will be the same.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Policy Improvement\n",
    "\n",
    "In the foregoing we have been using full backups for the dynamic programming algorithms. There is an alternative which can be both more efficient and can result in faster convergence. This concept is known as **generalized policy improvement** or GPI. GPI is applicable to both dynamic programming and reinforcement learning.\n",
    "\n",
    "The general idea of GPI is to break the policy improvement and evaluation steps into opposing processes and iterate between them. This process can be done in a quite granular way, even on a single state at a time. This makes the policy improvement algorithms much more scalable. The figure below illustrates the concept of GPI.  \n",
    "\n",
    "<img src=\"img/GPI.JPG\" alt=\"Drawing\" style=\"width:250px; height:250px\"/>\n",
    "<center> **Concept of Generalized Policy Improvement** </center>\n",
    "\n",
    "In GPI the policy improvement step makes the evaluation step less accurate and vice versa. This is what we mean by these steps acting in opposition. Over a number of iterations the gap between policy improvement and evaluation becomes narrower and eventually converges.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copyright 2018, Stephen F Elston. All rights reserved. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
