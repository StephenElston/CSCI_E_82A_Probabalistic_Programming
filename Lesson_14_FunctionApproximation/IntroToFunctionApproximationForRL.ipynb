{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Reinforcement Learning with Function Approximation\n",
    "\n",
    "## CSCI E-82A\n",
    "\n",
    "## Stephen Elston"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To date we have only considered reinforcement learning problems with a small number of **discrete states**. So far, we have applied **tabular algorithms** to the solution of RL problems. However, tabular methods will not work in cases where the tables are too large to be held in computer memory or where states are not discrete values. \n",
    "\n",
    "But, many practical problems have eight very large numbers of discrete states or have continuous states. Some example of such problems:  \n",
    "1. A Game such as chess or backgammon with a great number of possible board states. While the number of states is **countably finite** there are far too many for tabular solutions. For example, consider that each piece in a chess game can occupy any of 64 positions, and that there are anywhere between 32 pieces and 2 pieces on the board at a given time step. This situation leads to an explosion in possible states. Other games, such a Go, have far more states than chess.   \n",
    "2. A simple flight control system for a drone which has an infinite number of states in terms of 3-dimensional velocity, acceleration, and position, along with pitch yaw and role. All 12 of these variables have **continuous values** and, thus, an **infinite number of states**.   \n",
    "\n",
    "To address such problems we must have a better representations. In this lesson we will focus on a powerful class of representations known as **function approximation**. By using function approximation we can represent a large number of states, even an infinite number, with a limited number of **parameters**. Using function approximation problems of great complexity can be tackled, at least in theory.  \n",
    "\n",
    "A reinforcement learning agent using function approximation is illustrated schematically in the figure below. \n",
    "\n",
    "<img src=\"img/AgentEnvironment.JPG\" alt=\"Drawing\" style=\"width:500px; height:300px\"/>\n",
    "<center> **Reinforcement Learning Agent with Function Approximation Representation and Environment** </center>  \n",
    "\n",
    "In this lesson we will only address function approximation methods for **episodic** tasks. \n",
    "\n",
    ">**Suggested reading:** The following sections of Sutton and Barto, second edition, provide additional scope for the topics of this lesson: 9.1, 9.2, 9.3, 9.4, 9.5, 10.1, and 10.2.\n",
    "\n",
    ">**For further exploration:** The resources of [Open AI Gym](https://gym.openai.com/) provide a rich set of simulators along with a framework to help you construct and test RL algorithms. \n",
    "\n",
    ">**Code examples in depth:** *Deep Reinforcement Learning Hands-On* by Maxim Lapan, Packt, 2018, provides considerable detail on deep Q learning. The book contains in-depth examples and discusses important implementation details. The examples in this book use the PyTorch deep learning framework. \n",
    "\n",
    "In order to run all of the examples in this notebook you will need to install [Keras](https://keras.io/) and h5py; `pip install h5py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation for Non-Tabular RL\n",
    "\n",
    "The main question we will address in this lesson is what representations can we use when tabular methods will not work. The idea key idea is to **encode** a large, possibly infinite, number of states with a **few parameters**. Thus, **state values** or **action values** are represented by some **function of the state** of the environment.  \n",
    "\n",
    "There are, in fact, many possible approaches to this problem. Here we will focus on function approximation methods of which a few examples are: \n",
    "1. Simple **linear** and **polynomial** representations,\n",
    "2. **Fourier basis function** representations,\n",
    "3. **Course coding** using overlapping circular or elliptical regions,\n",
    "4. Various forms of **tile coding**,\n",
    "5. **Radial basis functions** or kernels,\n",
    "5. **Fully connected** deep neural networks, \n",
    "6. **Convolutional** deep neural networks. \n",
    "\n",
    "Each of the first five methods involve **coding** using some type of **basis function**. Basis functions rely on an implicit assumption that **nearby states have similar values**.  The representation is then **linear in the parameters** and **linear solution methods** are used to find these parameters. Linear methods have the advantage of computational efficiency. Further, at least for **on-policy** algorithms, **convergence is guaranteed**.     \n",
    "\n",
    "The deep neural networks provide a distinctly nonlinear function approximation method. Typically, deep neural networks are used for **off-policy Q-Learning** methods. The convergence properties of these algorithms can be problematic. In fact, there are few guarantees of convergence with **off-policy function approximation** algorithms.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tile coding\n",
    "\n",
    "Tile coding is a flexible and expressive function approximation method. The basic idea is simple. The **state space is divided** into small patches using a regular pattern of **geometric shapes** or **tiles**. The function approximation has one parameter (weight) for each tile.    \n",
    "\n",
    "An example tile coding of a two dimensional state space is shown in the figure below. In this case, a uniform 8x8 set of tiles are used, leading to a representation with 64 parameters. \n",
    "\n",
    "<img src=\"img/Tile1.JPG\" alt=\"Drawing\" style=\"width:300px; height:300px\"/>\n",
    "<center> **Two-dimensional state space encoded by 8x8 rectangular tiles** </center>\n",
    "\n",
    "In the above diagram the states shown by **X** are in the same tile and will be coded with the same parameter. The states show by **O** are in different tiles and are coded with different parameters. \n",
    "\n",
    "However, the tile codings are far from unique. Consider the tiling of the same state space shown in the figure below. In this case the tiles are a 4X16 grid. As in the first case, there are still 64 parameters. \n",
    "\n",
    "<img src=\"img/Tile2.JPG\" alt=\"Drawing\" style=\"width:300px; height:300px\"/>\n",
    "<center> **Two-dimensional state space encoded by 4x16 rectangular tiles** </center>\n",
    "\n",
    "In the above coding the states shown with **O** are in the same tile and represented by the same parameter. But, the states shown as **X** are now in different tiles and are represented by different parameters. \n",
    "\n",
    "A great many tile coding schemes are possible. Commonly, multiple tiling schemes are used simultaneously. This practice allows for the capture of information at **different scales**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep neural networks\n",
    "\n",
    "Deep neural networks are powerful function approximation methods. In most cases one of two deep neural network architectures are used:\n",
    "1. Fully connected network are used for cases where there are complex and highly nonlinear relationships between state values or action values. \n",
    "2. Convolutional networks are useful for cases where there is value coherency between adjacent states. Common examples include images and time series data.  \n",
    "\n",
    "In principle deep neural networks can approximate even highly complex nonlinear functions. Neural networks have been reviewed in a previous lesson. Here, we will just summarize some of the drawbacks of this attractive representation. \n",
    "1. Given the large number of parameters a large number of episodes are required for training. It is not unusual for several tens of millions of episodes to be required. Therefore, training time can be significantly longer than for other algorithms.\n",
    "2. As a result of the large number of parameters (high degrees of freedom) over-fitting is a constant problem. Careful attention must be paid to regularization methods. \n",
    "3. Deep neural network are known to have **brittle behavior**, wherein small changes in the input can result in surprising or unexpected predictions.\n",
    "\n",
    "The foregoing not withstanding, trained neural network models can be reasonably computationally efficient. In fact, prediction using trained neural network models is preformed routinely, even in embedded environments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Function and Stochastic Gradient Decent\n",
    "\n",
    "Following the discussion above, basis function methods are linear in their parameters. In the following we will express these parameters as a vector of model weights, $\\mathbf{w}$. We can then represent the approximate state value function with state $x(s)$ as:\n",
    "\n",
    "$$\\hat{v}(s,\\mathbf{w}) = \\mathbf{w}^T \\mathbf{x}(s) =  \\sum_{i=1}^d   v_i  x_i(s)$$\n",
    "\n",
    "In principle, **stochastic gradient decent** algorithms are an efficient way to solve for the weights, $\\mathbf{w}$. At each step the update is just:\n",
    "\n",
    "$$\\mathbf{w}_{t+1} = \\mathbf{w}_t + \\alpha\\ E_{\\hat{p}data}\\Big[ \\nabla_{w} J(\\mathbf{w}_t) \\Big]\\\\\n",
    "= \\mathbf{w}_t + \\alpha  \\big[v_{\\pi}(s) - \\hat{v}(S_t, \\mathbf{w}_t) \\big]\\nabla_w \\hat{v}(S_t,\\mathbf{w}_t)$$\n",
    "\n",
    "where, $\\hat{p}data$ is the mini-batch, and gradient is given by:   \n",
    "\n",
    "$$\\hat{v}(S_t,\\mathbf{w}_t) = \n",
    "\\begin{bmatrix} \n",
    "\\frac{\\partial \\hat{v}(S_t,\\mathbf{w}_t)}{\\partial w_1} \\\\\n",
    "\\frac{\\partial \\hat{v}(S_t,\\mathbf{w}_t)}{\\partial w_2} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial \\hat{v}(S_t,\\mathbf{w}_t)}{\\partial w_d}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "In many practical algorithms, $\\hat{v}(s)$ is a **bootstrapped** approximation. This means are error term and gradient are not exact. We call such algorithms **semi-gradient decent** methods as they use an approximation of the gradient. This approach generally works well, but does not have the strong convergence guarantees of stochastic gradient decent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Approximation Error\n",
    "\n",
    "As already noted, when tabular algorithms are not feasible, we must resort to function approximation. We can use function approximation for both state values, $\\hat{v}(s)$, and action values, $\\hat{q}(s,a)$. We should always keep in mind that we are dealing with **approximations** and will never know the true state values, $v_{\\pi}(s)$, and action values, $q_{\\pi}(s,a)$. There will always be some **error** between the true values and the approximated values. For example, for state value approximation we can express this error as the **mean squared value error**:\n",
    "\n",
    "$$\\overline{VE}(w) = \\sum_{s \\in S} \\mu(s) \\Big[ v_{\\pi}(s) -  \\hat{v}(s,\\mathbf{w}) \\Big]^2$$  \n",
    "\n",
    "Here, $\\mu(s)$ is a weight indicating home important the state $s$ is. For on-policy RL, $\\mu(s)$ is a probability known as the **on-policy distribution**. \n",
    "\n",
    "A similar error metric can be constructed for $\\hat{q}(s,a)$.  \n",
    "\n",
    "In practical terms there is a trade-off between the complexity of the approximate representation and the error. More complex representations require more parameters, but have lower error and vice versa. This situation is exactly analogous to the bias-variance trade-off, familiar from machine learning.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Mountain Car Problem\n",
    "\n",
    "The [mountain car problem](https://en.wikipedia.org/wiki/Mountain_car_problem) was first proposed in the [Andew Moore's Ph.D. dissertation (1990)](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.17.2654). The mountain car problem has become a canonical testbed for many reinforcement learning algorithms. \n",
    "\n",
    "In this problem an under-powered car must climb a steep hill. However, the car does not have sufficient engine power to climb the grade. The car must travel up another hill in order to gain sufficient speed (actually kinetic energy) to climb the large hill. \n",
    "\n",
    "The position, $x$, and velocity, $\\dot{x}$, of the car are the state variables. The updates of the state variables at each time step are determined by the following equations: \n",
    "\n",
    "$$x' = x + \\dot{x} \\\\\n",
    "\\dot{x}' = \\dot{x} + 0.001 * \\ddot{x} - 0.0025 * cos(3 * x)$$\n",
    "\n",
    "The object of this problem is to find the optimal acceleration given the car state to allow the car to get to the top of the hill. The car has three acceleration states, $\\ddot{x}$, which must be selected by the agent:\n",
    "\n",
    "$$\\ddot{x} = \\{ -1.0, 0.0, 1.0 \\}$$\n",
    "\n",
    "The position and velocity are bounded, with the goal at the upper bound of position:\n",
    "\n",
    "$$-1.2 \\le x \\le 0.5 \\\\\n",
    "-0.07 \\le \\dot{x} \\le 0.07$$\n",
    "\n",
    "The reward at each time step is -1.0 and the reward for reaching the goal is 100.  \n",
    "\n",
    "The car is randomly initialized using a uniform distribution:\n",
    "\n",
    "$$p(x_0) = uniform(-0.6 \\le x_0 \\le -0.4$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The mountain car problem is hard\n",
    "\n",
    "At first glance, the mountain car problem may seem like it should have an easy solution. However, looks are deceptive. The learning an optimal policy for this problem is difficult. In fact, conventional control theory approaches fail to provide solutions. Some reasons for this difficulty include:\n",
    "1. The non-linear coupling between the two state variables, which makes the state transitions between the infinite number of states hard to predict.\n",
    "2. The delayed reward which is only observed when the goal is achieved. This fact is common to many difficult RL problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation of car environment\n",
    "\n",
    "The code in the cell below **simulates the car environment**. Two functions are used by the agent to interact with the environment:\n",
    "1. The `sim_car` function returns a state transition and a reward, given the agent's current state and an action. In addition, a flag is returned to indicate if the goal has been reached.\n",
    "2. The `initialize_car` function returns a random starting position for the car within the specified bounds. \n",
    "\n",
    "Taken together, calls to these two functions define the **boundary between the agent and the environement**. Execute the code in the cell below to exercise these functions and examine the resulting plots for a case where the acceleration is set to 0. \n",
    "\n",
    ">**Note:** An Open AI Gym [environment simulator](https://gym.openai.com/envs/MountainCarContinuous-v0/) for the mountain car problem is available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from math import cos\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def sim_car(x, x_dot, acceleration, x_lims = (-1.2,0.5), x_dot_lims = (-0.07,0.07)):\n",
    "    ## Compute velocity within limits\n",
    "    x_dot_prime = x_dot + 0.001 * acceleration - 0.0025 * cos(3 * x)\n",
    "    if(x_dot_prime < x_dot_lims[0]): x_dot_prime = x_dot_lims[0]\n",
    "    if(x_dot_prime > x_dot_lims[1]): x_dot_prime = x_dot_lims[1]\n",
    "        \n",
    "    ## Now update position\n",
    "    x_prime = x + x_dot_prime\n",
    "    if(x_prime < x_lims[0]): x_prime = x_lims[0]\n",
    "    if(x_prime > x_lims[1]): x_prime = x_lims[1]\n",
    "      \n",
    "    ## At the terminal state or not and set reward\n",
    "    if(x_prime >= x_lims[1]): \n",
    "        done = True\n",
    "        reward = 100.0\n",
    "    else: \n",
    "        done = False\n",
    "        reward = -1.0\n",
    "        \n",
    "    return(x_prime, x_dot_prime, done, reward)    \n",
    "        \n",
    "def initalize_car(x_lims = (-0.6,-0.4)):\n",
    "    ## Find random start for car\n",
    "    return(nr.uniform(x_lims[0],x_lims[1]))\n",
    "\n",
    "## Test the function\n",
    "a = -0.0\n",
    "x_dot = [0.0]\n",
    "x = [initalize_car()]\n",
    "for i in range(100):\n",
    "    x_temp, x_dot_temp, done, reward = sim_car(x[i], x_dot[i], a)\n",
    "    x.append(x_temp)\n",
    "    x_dot.append(x_dot_temp)\n",
    "    \n",
    "def plot_car(x, x_dot):    \n",
    "    ## Plot car position\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(211)    \n",
    "    ax1.plot(x)\n",
    "    ax1.set_ylabel('Positon of car')\n",
    "    \n",
    "    ## PLot car velocity\n",
    "    ax2 = fig.add_subplot(212)  \n",
    "    ax2.plot(x_dot)\n",
    "    ax2.set_ylabel('Velocity of car')\n",
    "    ax2.set_xlabel('Time')\n",
    "    \n",
    "plot_car(x,x_dot)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With no acceleration applied, the car oscillates back and forth. The motion is not damped since the simulator includes no friction term. \n",
    "\n",
    "Next, execute the code in the cell below and observe the effect of using a constant positive acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test the function with negative acceleration\n",
    "a = 1.0\n",
    "x_dot = [0.0]\n",
    "#x = [initalize_car()]\n",
    "x = [0.3]\n",
    "for i in range(200):\n",
    "    x_temp, x_dot_temp, done, reward = sim_car(x[i], x_dot[i], a)\n",
    "    x.append(x_temp)\n",
    "    x_dot.append(x_dot_temp)\n",
    "    \n",
    "plot_car(x,x_dot) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, the motion of the car is periodic. However, the cycles are no longer symmetric since there is a bias caused by the positive acceleration. \n",
    "\n",
    "Finally, execute the code in the cell below to observe the effect of negative acceleration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test the function with negative acceleration\n",
    "a = -1.0\n",
    "x_dot = [0.0]\n",
    "#x = [initalize_car()]\n",
    "x = [0.3]\n",
    "for i in range(500):\n",
    "    x_temp, x_dot_temp, done, reward = sim_car(x[i], x_dot[i], a)\n",
    "    x.append(x_temp)\n",
    "    x_dot.append(x_dot_temp)\n",
    "    \n",
    "plot_car(x,x_dot) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The position state of the car, quickly hits the negative limit. It then oscillates periodically thereafter approaching, but not quite hitting the limit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heuristic mountain car solution\n",
    "\n",
    "While learning optimal policy for the mountain car problem is rather difficult, it is not to hard to find a good heuristic solution. There are two basic observations on which we can base a heuristic:\n",
    "1. The car should accelerate in the direction of the slope when traveling up hill to maximize the height to which it can climb. \n",
    "2. The car should accelerate in the opposite direction to the slope when traveling down hill to maximize the speed the car has for climbing the opposite hill. \n",
    "\n",
    "The code in the cell below implements this heuristic and plots the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_car_a(x, x_dot, a):    \n",
    "    ## Plot car position\n",
    "    fig = plt.figure(figsize = (6,6))\n",
    "    ax1 = fig.add_subplot(311)    \n",
    "    ax1.plot(x)\n",
    "    ax1.set_ylabel('Positon of car')\n",
    "    \n",
    "    ## PLot car velocity\n",
    "    ax2 = fig.add_subplot(312)  \n",
    "    ax2.plot(x_dot)\n",
    "    ax2.set_ylabel('Velocity of car')\n",
    "    \n",
    "    ## PLot acceleration\n",
    "    ax2 = fig.add_subplot(313)  \n",
    "    ax2.plot(a)\n",
    "    ax2.set_ylabel('Acceleration of car')\n",
    "    ax2.set_xlabel('Time')\n",
    "\n",
    "    \n",
    "def heuristic_control(x, x_dot):\n",
    "    ## Set acceleration to +1, -1 or 0 based on postion and velocity\n",
    "    if((x > 0.0 and x_dot > 0.0) or (x < 0.0 and x_dot > 0.0)): a = +1.0\n",
    "    elif((x < 0.0 and x_dot < 0.0) or (x > 0.0 and x_dot < 0.0)): a = -1.0\n",
    "    else: a = 0.0\n",
    "    return(a)    \n",
    "\n",
    "        \n",
    "## Initialize the car state\n",
    "a = [0.0]\n",
    "x_dot = [0.0]\n",
    "x = [initalize_car()]\n",
    "print('Starting positon = ' + str(x[0]))\n",
    "\n",
    "## Iterate until termination\n",
    "i = 0    \n",
    "done = False\n",
    "while not done:\n",
    "    ## Update the position and velocity    \n",
    "    x_temp, x_dot_temp, done, reward = sim_car(x[i], x_dot[i], a[i])\n",
    "    x.append(x_temp)\n",
    "    x_dot.append(x_dot_temp)\n",
    "        \n",
    "    ## Update acceleration using the heuristic\n",
    "    i = i + 1    \n",
    "    a.append(heuristic_control(x[i], x_dot[i]))\n",
    "        \n",
    "plot_car_a(x,x_dot,a)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the plots produced. The car position chart shows a fairly quick convergence to the goal state. The acceleration chart shows the heuristic policy is followed. However, this cannot be considered an optimal policy. Notice that the car stays at nearly constant position of about -1.2 for a number of time steps. Nothing was gained in terms of achieving the goal in this period. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tile Coding N-Step SARSA Solution for Mountain Car Problem\n",
    "\n",
    "Now, we will apply tile coding along with N-step SARSA to the solution of the mountain car problem. Using tile coding results in a representation linear in the weights for each tile. \n",
    "\n",
    "### Tile coding the state variables\n",
    "\n",
    "There are two state variables for the mountain car problem, position and velocity. In this case, we will use a 10x10 grid of uniform tiles for each of the 3 actions. This coding results in **300 weights** representing the **action values** for each possible combination of the state variables. \n",
    "\n",
    "The code in the cell below computes the tile assignment for the velocity state variable, $x$. The range of possible values within the velocity limits is divided into 10 equal segments. Execute the code and examine the results of the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_state(x, x_lims = (-1.2,0.5), n_tiles = 10):\n",
    "    \"\"\"Function to compute tile state given positon\"\"\"\n",
    "    state = int((x - x_lims[0])/(x_lims[1] - x_lims[0]) * float(n_tiles))\n",
    "    if(state > n_tiles - 1): state = n_tiles - 1\n",
    "    return(state)\n",
    "\n",
    "for x in list(np.linspace(-1.2,0.5,20)):\n",
    "    print('x = ' + str(x) + ' state = ' + str(x_state(x)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below tile encodes the velocity state variable into 10 equal segments between the limits. Execute this code and examine the results of the test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_dot_state(x_dot, x_dot_lims = (-0.07,0.07), n_tiles = 10):\n",
    "    \"\"\"Function to compute tile state given velocity\"\"\"\n",
    "    state = int((x_dot - x_dot_lims[0])/(x_dot_lims[1] - x_dot_lims[0]) * float(n_tiles))\n",
    "    if(state > n_tiles - 1): state = n_tiles - 1\n",
    "    return(state)\n",
    "\n",
    "for x in list(np.linspace(-0.07,0.07,20)):\n",
    "    print('x_dot = ' + str(x) + ' state = ' + str(x_dot_state(x)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two functions provide a lookup method for the index of the tile along the two state variable dimensions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Step SARSA for Tile Coding\n",
    "\n",
    "The code in the cell below implements N-step SARSA for the simple tile coding scheme. The N-step SARSA learns the weights for the tiles. Technically, this is a **semi-gradient decent** algorithm since bootstrapped values are used for the weight updates. \n",
    "\n",
    "The **approximate action value** for a given state and action, $x_i(s,a)$, given the $d$ tile weights, $w_i$, can be computed as:\n",
    "\n",
    "$$\\hat{q}(s,a,\\mathbf{w}) = \\sum_{i=1}^d  w_i  x_i(s,a)$$\n",
    "\n",
    "The state action variables, $x_i(s,a)$, are just binary indicators:\n",
    "\n",
    "$$x_i(s,a) = 1\\ if\\ in\\ tile\\ i\\\\\n",
    "x_i(s,a) = 0\\ otherwise$$\n",
    "\n",
    "The weight update for the N-step SARSA tile coding algorithm, given rewards $R_t$, the N-step bootstrapped return is expressed as:  \n",
    "\n",
    "$$G_{t:t+n} = R_{t+1} + \\gamma R_{t+2} + \\ldots + \\gamma^{n-1} R_{t+n} + \\gamma^n \\hat{q}(S_{t+n}, A_{t+n}, w_{t+n-1})$$\n",
    "\n",
    "Using this return, the action value update becomes:\n",
    "\n",
    "$$w_{t+n}(S_t, A_t) = w_{t+n-1} + \\alpha \\big[ G_{t:t+n} -\\hat{q}(S_{t+n}, A_{t+n}, w_{t+n-1}) \\big]\\ \\nabla\\hat{q}(S_{t}, A_{t}, w_{t+n-1})$$\n",
    "\n",
    "Where   \n",
    "$\\delta_t =  G_{t:t+n} - \\hat{q}(S_{t+n}, A_{t+n}, w_{t+n-1}= $ the n-step TD error, and   \n",
    "$\\nabla\\hat{q}(S_{t}, A_{t}, w_{t+n-1} = $ the gradient of the action value approximation. \n",
    "\n",
    "Given the linear relationship between $\\hat{q}(S_{t}, A_{t}, w_{t+n-1}$ and $w_{t+n-1}$ the gradient is easy to compute: $\\nabla\\hat{q}(S_{t}, A_{t}, w_{t+n-1} = 1$ for cases where $x_i(s,a) = 1$.\n",
    "\n",
    "In the algorithm presented, actions are selected using an $\\epsilon$-greedy approach. \n",
    "\n",
    "As with other N-step SARSA algorithms there as a bit of bookkeeping. Further details of this algorithm can been obtained by reading the code comments. \n",
    "\n",
    "Execute this code and examine the results of the test case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def tile_SARSA(episodes = 1000, gamma = 0.9, epsilon = 0.05, alpha = 0.02, \n",
    "               n = 4, goal = 0.5, a_knot = 0.0, x_dot_knot = 0.0):\n",
    "    ## Possible actions\n",
    "    actions = [-1.0,0.0,1.0]\n",
    "    \n",
    "    ## Initialize the weight array for action, position, velocity\n",
    "    w = np.zeros((3,10,10))\n",
    "\n",
    "    ## Loop over the episodes\n",
    "    for _ in range(episodes):\n",
    "        ## Initialize the car state\n",
    "        a = [a_knot]\n",
    "        a_index = 1\n",
    "        x_dot = [x_dot_knot]\n",
    "        x_dot_index = x_dot_state(x_dot[0])\n",
    "        x = [initalize_car()]\n",
    "        x_index = x_state(x[0])\n",
    "        \n",
    "        t = 0 # Initialize the time step count\n",
    "        T = float(\"inf\")\n",
    "        tau = 0\n",
    "        reward_list = [] \n",
    "        \n",
    "        done = False\n",
    "        i = 0 # Index for accumulting position and velocity steps\n",
    "        while(not done):\n",
    "            if(t < T):\n",
    "                ## Update postion and velocity of the car given action and get the reward\n",
    "                x_prime, x_dot_prime, done, reward = sim_car(x[i], x_dot[i], a[i])\n",
    "                reward_list.append(reward)  # append the reward to the list\n",
    "        \n",
    "                ## The next state given the action\n",
    "                x_prime_index = x_state(x_prime)\n",
    "                x_dot_prime_index = x_dot_state(x_dot_prime)\n",
    "        \n",
    "            if(done): T = t + 1  # We reached the terminal state\n",
    "            else:\n",
    "                # Select and store the next action using the policy with epslion greedy approach\n",
    "                if(nr.uniform() > epsilon):\n",
    "                    a_prime_index = np.where(w[:,x_prime_index,x_dot_prime_index] == np.max(w[:,x_prime_index,x_dot_prime_index]))\n",
    "                    ## break the tie if needed\n",
    "                    if(a_prime_index[0].shape[0] > 1): a_prime_index = nr.choice(a_prime_index[0])\n",
    "                    else: a_prime_index = a_prime_index[0][0] \n",
    "                    a_prime = actions[a_prime_index]\n",
    "                else:\n",
    "                    a_prime_index = nr.choice(range(3)) #[0]\n",
    "                    a_prime = actions[a_prime_index]\n",
    "           \n",
    "            tau = t - n + 1 ## update the time step being updated\n",
    "  \n",
    "            if(tau >= 0): # Check if enough time steps to compute return\n",
    "                ## Compute the return\n",
    "                ## The formula for the first index in the loop is different from Sutton and Barto\n",
    "                ## but seems to be correct at least for Python.\n",
    "                G = 0.0 \n",
    "                for j in range(tau, min(tau + n, T)):\n",
    "                    G = G + gamma**(j-tau) * reward_list[j]   \n",
    "                ## Deal with case of where we are not at the terminal state\n",
    "                if(tau + n < T): G = G + gamma**n * w[a_prime_index,x_prime_index,x_dot_prime_index] \n",
    "                ## Finally, update w\n",
    "                w[a_index,x_index,x_dot_index] = w[a_index,x_index,x_dot_index] + alpha * (G - w[a_index,x_index,x_dot_index])        \n",
    "    \n",
    "            ## Set action and state for next iteration\n",
    "            if(x_prime <= goal):\n",
    "                a_index = a_prime_index\n",
    "                x_index = x_prime_index\n",
    "                x_dot_index = x_dot_prime_index  \n",
    "                a.append(a_prime) \n",
    "                x.append(x_prime)\n",
    "                x_dot.append(x_dot_prime)\n",
    "                i = i + 1\n",
    "\n",
    "            ## increment t\n",
    "            t = t + 1\n",
    "    return(w)  \n",
    "\n",
    "print(np.round(tile_SARSA(), 2))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 300 weight values represented in the above array. Notice that generally the weights, and thus the action values, increase with larger values of position which are closer to the goal. \n",
    "\n",
    "The code in the cell below uses the results of N-step SARSA to compute a policy. The action is selected in a greedy manner based on the action values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def tile_SARSA_policy(episodes = 1000, gamma = 0.9, epsilon = 0.05, alpha = 0.02, \n",
    "                   n = 4, goal = 0.5, a_knot = 0.0, x_dot_knot = 0.0):\n",
    "    ## Possible actions\n",
    "    actions = [-1.0,0.0,1.0]\n",
    "    \n",
    "    ## Evaluate policy with SARSA\n",
    "    Q = tile_SARSA(episodes = episodes, gamma = gamma, epsilon = epsilon, alpha = alpha, \n",
    "                         n = n, goal = goal, a_knot = a_knot, x_dot_knot = x_dot_knot)\n",
    "    Q_shape = Q.shape\n",
    "    \n",
    "    ## Array to hold policy\n",
    "    policy = np.zeros((Q_shape[1],Q_shape[2]))\n",
    "    \n",
    "    ## Loop over postions\n",
    "    for x_index in range(Q_shape[1]):\n",
    "        ## Loop over velocity\n",
    "        for x_dot_index in range(Q_shape[2]):\n",
    "            ## Find the index with max Q\n",
    "            a_index = np.where(Q[:,x_index,x_dot_index] == np.max(Q[:,x_index,x_dot_index]))\n",
    "            ## Break the tie by using 0 acceleration\n",
    "            if(a_index[0].shape[0] > 1 ): a_index = 1\n",
    "            else: a_index = a_index[0][0]    \n",
    "            \n",
    "            ## Now fill in the policy array\n",
    "            policy[x_index,x_dot_index] = actions[a_index]\n",
    "                    \n",
    "    return(policy)                    \n",
    " \n",
    "\n",
    "\n",
    "SARSA_Tile_Policy = tile_SARSA_policy(episodes = 5000)   \n",
    "SARSA_Tile_Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is an array with actions specified for each tile in the coding scheme. This policy is not truly optimal since we have limited the number of episodes to 5,000. \n",
    "\n",
    "We can get a better felling for how this policy changes with state (position and velocity) by plotting the policy array. Execute the code in the cell below and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_policy(policy):\n",
    "    plt.imshow(policy)\n",
    "    plt.xlabel('Position')\n",
    "    plt.ylabel('Velocity')\n",
    "    plt.title('Acceleration given position and velocity')\n",
    "    \n",
    "display_policy(SARSA_Tile_Policy)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This display shows that the policy is not really optimal. For the most part, positive acceleration is applied when the case is traveling toward the goal. However, in some cases negative acceleration is applied. Further, negative acceleration is not applied in all the cases one should expect, and there are many states for which 0 acceleration is used. \n",
    "\n",
    "The code in the cell below applies the computed policy to one episode of the mountain car problem. The car is started with a random initial velocity and the policy is followed until the goal is achieved. Charts of results are then plotted. Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize the car state\n",
    "a = [0.0]\n",
    "x_dot = [0.0]\n",
    "x = [initalize_car()]\n",
    "print('Starting positon = ' + str(x[0]))\n",
    "\n",
    "## Iterate until termination\n",
    "i = 0    \n",
    "done = False\n",
    "while not done:\n",
    "# for _ in range(1000):    \n",
    "    ## Update the position and velocity    \n",
    "    x_temp, x_dot_temp, done, reward = sim_car(x[i], x_dot[i], a[i])\n",
    "    x.append(x_temp)\n",
    "    x_dot.append(x_dot_temp)\n",
    "    \n",
    "    ## Get the indices\n",
    "    x_index = x_state(x[i])\n",
    "    x_dot_index = x_dot_state(x_dot[i])\n",
    "        \n",
    "    ## Update acceleration using the heuristic\n",
    "    i = i + 1    \n",
    "    a.append(SARSA_Tile_Policy[x_index, x_dot_index])\n",
    "        \n",
    "plot_car_a(x,x_dot,a)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The car reaches the goal in a reasonable number of time steps, however, it is clear from these charts this is not an optimal policy. For example, notice how many time steps have no acceleration applied. \n",
    "\n",
    "If you have time you can run the above code blocks for 10,000, 20,000 or even larger numbers of episodes. You should see an notable improvement in the policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep NN Function Approximation for the Mountain Car Problem\n",
    "\n",
    "A general, but computationally intensive, solution to the mountain car problem can be obtained using **deep Q-Learning**. Here we use a neural network model as a function approximator for the action values, $\\hat{q}(S_{t}, A_{t}, w_{t-1})$.   \n",
    "\n",
    "The code in the cell below defines a simple neural network model to approximate the action values for the mountain car problems. A few key points include:\n",
    "- There are three input variables, the two state variables, position and velocity, along with the action. \n",
    "- A single hidden layer of 20 units is used. This is a rather spare representation, but is chosen for convenience of the demonstration rather than accuracy. \n",
    "- Since over-fitting is a constant problem with neural networks three regularization methods are applied, l2 regularization, dropout regularization and early stopping. \n",
    "- Since this is a regression problem, to approximate the numeric action value, the output layer consists of a single unit with linear activation. \n",
    "\n",
    "Execute this code to run the simple test case.\n",
    "\n",
    ">**Note:** The code shown in this notebook is intended to illustrate the basic concepts of deep Q-Learning. For more robust solutions you are advised to look at the contributed [keras-rl package](https://github.com/keras-rl/keras-rl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.utils.np_utils as ku\n",
    "import keras.models as models\n",
    "import keras.layers as layers\n",
    "from keras.layers import Dropout\n",
    "from keras import regularizers\n",
    "\n",
    "def DL_model():\n",
    "    ## Define the Keras model\n",
    "    function_approx = models.Sequential()\n",
    "    function_approx.add(layers.Dense(3, activation = 'relu', input_shape = (3,), \n",
    "                                     kernel_regularizer=regularizers.l2(0.01)))\n",
    "    function_approx.add(Dropout(0.5)) # Use 50% dropout\n",
    "    function_approx.add(layers.Dense(20, activation = 'relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "    function_approx.add(Dropout(0.5)) # Use 50% dropout\n",
    "    function_approx.add(layers.Dense(1, activation = 'linear'))\n",
    "    function_approx.compile(optimizer = 'rmsprop', loss = 'mse', metrics = ['mae'])\n",
    "    ## Define the callback list for early stopping   \n",
    "    filepath = 'my_model_file.hdf5' # define where the model checkpoints are saved\n",
    "    callbacks_list = [keras.callbacks.EarlyStopping( #monitor = 'val_loss', patience = 1)\n",
    "            monitor = 'val_loss', # Use loss to monitor the model\n",
    "            patience = 1 # Stop after one step with lower accuracy\n",
    "        ),\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            filepath = filepath, # file where the checkpoint is saved\n",
    "            monitor = 'val_loss', # Don't overwrite the saved model unless val_loss is worse\n",
    "            save_best_only = True # Only save model if it is the best\n",
    "        )\n",
    "    ]\n",
    "    return(function_approx)\n",
    "function_approx = DL_model()\n",
    "function_approx.predict(np.array([[0.1,0.1,0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below preforms greedy selection of the next action. The neural network is used to predict the action value for each possible action and the maximum is selected. Execute this code to run the simple test case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_Q(x,x_dot,actions,model):\n",
    "    Q = []\n",
    "    ## Iterate over all actions to find Q values\n",
    "    for i,a in enumerate(actions):\n",
    "        Q.append(model.predict(np.array([[x,x_dot,a]]))[0][0])\n",
    "    ## Find the action with max Q\n",
    "    max_index = np.argmax(Q)\n",
    "    return(Q[max_index], actions[max_index], max_index)\n",
    "        \n",
    "actions = [-1.0,0.0,1.0]        \n",
    "max_Q(0.1,0.0,actions,function_approx)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below execute a simple Q-Learning algorithm using the previously defined neural network model as the function approximator. Some important points for this algorithm are:\n",
    "1. An $\\epsilon$-greedy approach is used to select actions. \n",
    "2. The target action value,  $\\hat{q}(S_{t}, A_{t}, w_{t-1})$, used to train the neural network model is the 1-step bootstrapped reward estimate. \n",
    "\n",
    "Some additional details of the algorithm can be learned by reading the code comments. \n",
    "\n",
    "Execute this code for just 10 episodes and examine the results. This will take some time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def deep_Q(episodes = 1000, gamma = 0.9, epsilon = 0.05, alpha = 0.02, \n",
    "               n = 1, goal = 0.5, a_knot = 0.0, x_dot_knot = 0.0):\n",
    "    ## Possible actions\n",
    "    actions = [-1.0,0.0,1.0]\n",
    "    \n",
    "    ## Define the Keras model\n",
    "    function_approx = DL_model()\n",
    "\n",
    "    ## Loop over the episodes\n",
    "    for _ in range(episodes):\n",
    "        ## Initialize the car state\n",
    "        a = a_knot\n",
    "        x_dot = x_dot_knot\n",
    "        x = initalize_car()\n",
    "        x_list = []\n",
    "\n",
    "        done = False\n",
    "        i = 0 # Index for accumulting position and velocity steps\n",
    "        while(not done):\n",
    "            ## Update postion and velocity of the car given action and get the reward\n",
    "            x_prime, x_dot_prime, done, reward = sim_car(x, x_dot, a)\n",
    "            # Select and store the next action using the policy with epslion greedy approach\n",
    "            if(nr.uniform() > epsilon):\n",
    "                Q, a_prime, a_prime_index = max_Q(x_prime,x_dot_prime,actions,function_approx)\n",
    "            else: # Time to explore\n",
    "                a_prime_index = nr.choice(range(3)) #[0]\n",
    "                a_prime = actions[a_prime_index]\n",
    "                Q = function_approx.predict(np.array([[x_prime,x_dot_prime,a]]))[0][0]\n",
    "           \n",
    "            ## Compute the bootstrap estimate of Q. This s our target for re-fitting the\n",
    "            ## the NN regression model.\n",
    "            G = reward + gamma**n * function_approx.predict(np.array([[x_prime,x_dot_prime,a_prime]]))[0][0]\n",
    "            ## Finally, refit the model with the new target.\n",
    "            function_approx.fit(x = np.array([[x,x_dot,a]]), y = np.array([G]), epochs=1, verbose=0)\n",
    "            \n",
    "            ## Set action and state for next iteration\n",
    "            if(x_prime <= goal):\n",
    "                x = x_prime\n",
    "                x_dot = x_dot_prime\n",
    "                a = a_prime\n",
    "                i = i + 1\n",
    "                x_list.append(x)\n",
    "                \n",
    "        print('iterations = ' + str(i))\n",
    "    \n",
    "    return(function_approx)  \n",
    "\n",
    "deep_Q(episodes = 10)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of time steps required to reach the goal for each episode seems reasonable. However, as you have, no doubt noticed, this time required for even these 10 episodes is rather lengthy. In reality, thousands or tens of thousands of episodes are likely required to find a good approximation for the action values. This approximation would then be used to find a greedy policy given state. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Better Off-Policy Function Approximation Algorithms\n",
    "\n",
    "In the foregoing example, we have used a rather naive approach to off-policy reinforcement learning with function approximation. Much better algorithms exist, including **eligibility trace** methods and **policy gradient** algorithms. Unfortunately, these algorithms are beyond the scope of this course. Additional information on these algorithms can be found in chapters 12 and 13 of Sutton and Barto, second edition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copyright 2018, Stephen F. Elston. All rights reserved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
